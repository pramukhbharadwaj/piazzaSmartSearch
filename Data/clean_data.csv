Post,Sentence
689,"quiz. 9 - neural language models use word-embedding models in their training..hi , when taught in class it was mentioned that a by product of neural language models was the word embeddings but i do not remember word embeddings to be involved in training. please can someone confirm this. thanks"
689,"yes, word embedding are not used to rain word2vec. we only use one-hot vectors and then learn word embeddings as the weights of the two-layer network."
663,regarding random shuffling between epochs while training.i am getting better mean f1 scores for both vanilla and averaged perceptron without random shuffling the dataset between epochs while training. is shuffling between epochs mandatory while training? can i train the perceptron without shuffling the dataset?
663,"not mandatory, but see @661"
638,"feature extraction and classifying using one model.since i use tfidf for feature extraction, i need to keep the bag of words in the model for feature extraction in classifying right? does that mean i also need to store the idf for each word calculated in training and load it for classification? since the testing data need the same idf for the words in my bag of words. (unseen words will be discarded)"
638,you could. another option would be to put the idf weights into your leaned weight vectors.
608,"feature vector for unseen words.as we are creating one-hot encoding to convert reviews into features, how can i handle any unseen words in the test dataset? should i skip the word? or is there any other better alternative?"
608,"ignoring an unknown word is probably the simplest solution. alternatively, you can devise features that could be triggered by both known and unknown words."
579,"smoothing transition probability matrix.what is meant by smoothing each row in the transition probability matrix? if we are generally dividing by the number of times the previous tag is in the corpus, are we adding 1 to that number?"
579,"laplace smoothing was covered in the first week of class @490, and again last week (lecture 13 on october 4). smoothing each row just means treating each outgoing transition separately. that is, we perform smoothing on all the transitions from noun; separately, we perform smoothing on all the transitions from verb; and so on. because each of these is a separate probability distribution."
541,hmm model.do we have to use forward-backward algorithm to learn the model as shown in the reference?
541,"the forward-backward algorithm is for unsupervised learning. you do not need to do that, because the exercise provides labeled training data."
536,"smoothing.instead of laplace smoothing, i am trying to use additive smoothing. still, i am not able to get good accuracy, leading not to be able to pinpoint the exact additive integer. is there any other way to do this?"
536,"sophisticated smoothing could lead to a small improvement, but if the accuracy you are getting is not in the general range of the baseline or reference, then you probably have a bug or error, and finding and fixing that would lead to more improvement than fiddling with the smoothing method"
589,"how to use word tokens as features.in the overview of code assignment 4, it said that ""you may use the word tokens as features. "". professor said i can count words instead of using tf-idf. can anybody give me an example about how to transfer a review (i.e. a sequence of words/strings) into a vector (a sequence of numbers)?"
589,"using words as features, each word can be considered a feature, and the count of the word is the value. so a review like ""the omni is the best hotel in the world"" will have the following feature values: {'best': 1, 'hotel': 1, 'in': 1, 'is': 1, 'omni': 1, 'the': 3, 'world': 1} if you want to think of this as a fixed-length vector, then the length would be the entire vocabulary, and most values will be zero. notice how in the feature structure above, i silently changed 'the' to 'the', but kept 'omni' without change. this would be difficult to do programmatically, but this is one of the things to experiment with: is it helpful to change case? mess around with suffixes? do something with punctuation? is it helpful to develop features that are not words? for example, use word bigrams as features? are there any words that should be excluded from being used as features? i recommend starting with very simple features (say, just words), in order to get the basic logic of the perceptron working, once you have that, you can experiment to see how changes to features affect performance of the perceptron. no, word2vec is not allowed for this assignment."
528,smoothing emission probability.why are we not smoothing emission probability?
528,"it is a tradeoff of runtime and accuracy. my best recommendation is to try it both ways: implement your model with and without smoothing on the emissions, measure runtime and accuracy for both options, and decide if the tradeoff is worthwhile. looking at the theory, smoothing the emission probabilities is expected to cause a substantial increase in runtime, because now you are calculating paths into every tag for every word, instead of throwing away most of the paths, it also has the potential for an improvement in accuracy, since you may run into a situation where context strongly suggests a tag but the current word was only seen with other tags. however, the improvement in accuracy is likely to be fairly modest, because if a word was seen with some tags but not others, then either the word is rare or the other tags are rare for that word."
511,states vs observations.the parts of speech themselves are the states correct? which would make each word the observation? i am confused because there is a possibility our input data does not contain all possible part of speech tags and therefore will always incorrectly label any words belonging to that part of speech. is there a known list of part of speech tags we are supposed to use? or just use the ones from the training data?
511,"yes and yes. i am not sure if the data we are working with will have such cases. but if we do, then you are correct. it can happen that we do not see a tag in train dataset because of which we assign words belonging to that tag incorrectly when tagging test dataset. no there is no known list of part of speech tags. we should be using and learning model from the ones that are present in training data."
506,is brill tagger a discriminative model or generative model?.
506,neither. the model does not output probabilities of tags or sequences of tags.
497,"finding open-class parts of speech.i am trying to find a good way to determine if a part of speech can be considered open-class. are there any general rules to follow when doing this? i tried calculating the average vocabulary size of each part of speech, and then classifying everything with a greater than average vocab size as open-class. however, this greatly reduced the algorithms accuracy, so i suspect this is the wrong way to go about it."
497,"the open/closed class distinction should not greatly improve or greatly decrease accuracy. remember, it only applies to unseen words, and there are not that many of those, so the effect should be rather small. from a good implementation of the open/closed class distinction you can expect an improvement of about 1 percentage point. if you see a great reduction in accuracy, then my first suspicion would be a bug in the implementation."
490,"hmm smoothing.hi! i went through the algorithm in the references given to us and although there are tables showing the viterbi trellis for an example with and without smoothing, i do not particularly see how this smoothing is done. are we allowed to look for references on how smoothing is applied to the computation of transition probabilities or can you point some of the references we are allowed to use to this end? thanks!"
490,"smoothing was covered in the first week of class, when talking about naive bayes. you can see descriptions of the process in jurafsky and martin, chapter 4, pages 5–6, and in manning, raghavan and schutze, chapter 13, page 260. smoothing for hmm works exactly the same way. you just need to make sure to separately smooth each row in the matrix, since each row represents an independent probability distribution. you are allowed to consult references. what is not allowed is consulting implementations (or texts that specifically describe an implementation)."
484,"question about initial probabilties.are initial probabilities calculated considering only the first word in a sentence, or do we use the markov assumption and consider each word in the sentence as a potential starting point? if it is the latter, then would every word in the sentence be considered as a potential starting point except the very last word in the sentence?"
484,"by initial probabilities, i assume you mean what jurafsky and martin call “an initial probability distribution over states” (top of page 9). this is the probability distribution of the first state in a sentence. to estimate the initial probability of state x, just take the number of sentences that start with x, and divide by the total number of sentences. do not forget to apply smoothing. an alternative approach is the algorithm presented in the written exercises that were distributed on blackboard. add a special initial state that is not associated with any observations, and assume that each sentence starts with a transition from that initial state to the first word-producing state. then the initial probability distribution from the previous paragraph becomes just another row in the transition matrix. the two approaches are mathematically equivalent."
443,"can anyone explain q1?.why chi-squared test is the most suitable hypothesis test to know if gender has anything to do with political party preference (democrats, republicans, etc)? why not t-test? what is the difference between chi-squared test and t-test?"
443,"the chi-squared test is used when we have categorical variables (here gender and political party preference). however, the t-test is used when we have quantitative variables (like to compare the mean of 2 samples)."
348,"using cuda.when implementing fnn and rnn, do we have to cast tensors to a cuda type to run on gpu? thank you!"
348,yes
254,"quiz question - 9.hi, question 9: choice 'it is efficient to learn long-term dependencies through time' is considered as a wrong option in the rubrics. but rnn's are known for long term dependencies over time through their dynamic system. normalized rnn's are even more capable of unfolding long term dependences over time. here are some relevant papers: 1. 2. 3."
254,"the key is the word ""efficient"". rnn can model the long-term dependencies but computationally the process is not efficient and that is why gated rnn is developed."
245,"not using grad.zero() to initialize the gradients after each iteration?.if we do not use grad.zero() and allow the gradients to accumulate, can it be useful in some cases?"
245,i have not seen such a case personally but likely there is a reason behind the pytorch designer team that set it to be accumulative.
241,"vague quiz 3 question.during the quiz there was this t/f question that i felt was vague:in rnn, the weights during each time step is shared.i thought that this question was asking if the neurons are using the same weights at each time step, which i thought was false since we should be updating the weights and hence, using different weights at each time step. the correct answer for this question was true. i believe that this question is asking whether we are calculating the new weight at time t using the weights from time t-1. am i the only who felt that the question was a little misleading?"
241,"weights during rnn backprop are not updated on each time step, since backprop happens after the full forward computation is done (i believe from slide 9 of lecture 6). that means that the same weights would actually be shared. therefore, i do not think this question was misleading. (for rnns, a ""timestep"" refers to the position of an individual rnn cell, not a full forward-backward computation.)"
227,"cosine similarity numerical.i just wanted to know if this is correct question : find cosine similarity for : x1 = (3,4) x2 = (1,0) solution : x*y cosine similarity = ________ ||x|| ||y|| 3*1 + 4*0 ___________________________ = 3/5 = 0.6 sqrt((3*3+4*4)x(1*1+0*0)) can you please confirm this calculation or is it incomplete as do we consider the euclidean distance instead, by assuming center to be the origin? (3-0)*(1-0) + (4-0)*(0-0) ___________________________ = 3/2 = 0.6 sqrt((3*3+4*4)x(1*1+0*0)) do we consider the center to be (0,0) always, as it was not mentioned by default in the question in the quiz? thank you"
227,the first computation is correct and the definitions of dot product and vector norm are both independent of the center of coordinate.
85,tf-idf.should i run tf-idf and extract features treating entire dataset as corpus? or by doing it classwise??
85,you should do it on the entire dataset.
84,"gradient descent vs perceptron algorithms.i want to verify my understanding is correct. the perceptron algorithm describes the full process of assigning random weights, then viewing examples from training data, adjusting weights, etc. gradient descent is a specific optimization technique that can be used to adjust weights. so one could replace the ""vanilla"" method of adjusting weights from the perceptron algorithm with gradient descent?"
84,"gradient descent is a general optimization algorithm and works for all functions that are differentiable. so, you can use it for many problems. it may be possible to come up with a solution for perception using gradient descent, too. perceptron algorithm is specific to the perception model."
79,"lemmatization based on pos tag.the ntlk lemmatizer will not lemmatize ""wearing"" to ""wear"" unless explicit mentioned to considered it as a word, because the default tag is ""noun"" . is this lemmatization acceptable?"
79,"i think so. you do not have to look at the specific cases of lemmatization otherwise it would be too much work, the default should suffice."
58,why do we use perplexity and not raw probability to evaluate language models?.why do we use perplexity and not raw probability to evaluate language models?
58,"we will cover this topic when we cover language models. in short, because raw probability values are between 0 and 1 and does not convey an intuitive measure."
39,"bow dictionary.does the dictionary need to be ordered in some way for bow to be used ? if so, is there any advantage to ordering it in alphabetical way ?"
39,"ordering should not have any effect when building bow. unless you are dealing with billions of reviews, even in such case ordering your bow w.r.t word frequency( to reduce document traversal) might not have any significant effect."
20,bag of words.how is a model which encounters a vector space representation of word vs document which is extremely sparse affected? what can be done to handle the negative effects on the model?
20,"in this case, the feature might not be a good representation of the document. you can expand the vocabulary to make the representation less sparse, at the cost of increasing the dimension of the vector. bow is a simple model, it suffers from the problems like the one you mentioned inherently. it is useful in some simple applications, when you do not have to deal with the outliers."
22,tf-idf.why do we use a logarithm for idf? how does it benefit mathematics
22,"it suppresses large values to avoid making some feature values that correspond to infrequent ones too large. in practice, this has impression been found to be helpful."
37,tokenize review before lemmatization.should we tokenize review_body before lemmatizing the sentence?
37,lemmatization is performed on a word-level basis.
86,contractions library.do you all have any other recommendations besides the standard contractions library (with which you can do contractions.fix)? i noticed online that there are issues with it when dealing with ambiguity in tense and all. or do you all think this can be safely ignored with respect to the performance of the various models?
86,"the standard library should be fine for a decent performance but if you think you can do better, you can use an improved approach on top of it."
101,performing lemmatization on the whole of reviews dataset.the lemmatization process is taking a long time. has anyone faced this issue? anything that can done to improve the runtime?
101,"running time depends on your machine, too. but generally, it is not superfast and we do not penaliz you if your running time is long."
116,"different preprocessing steps for each algorithm.hello, can we have different preprocessing steps for different algorithms given we explain the steps in detail in report and including/excluding some preprocessing steps improved scores. or is it required that all data go through same set of preprocessing steps and then each algorithm is evaluated on it?"
116,"yes, that is possible."
317,"what is the number of fnn output layer.like we set first hidden layer as 50, the second hidden layer is 10. but how about the output layer? is it 5 or 1?"
317,it should be 5 since it is a 5 class classification problem.
347,"input to rnn.will the input of the rnn be a word embedding aka a vector of size (300,)? if yes then would not hidden state also have to be 300 ?"
347,"i suppose the hidden layer to have a shape like (num_layers, time_steps(i.e. 20), output_embedding) now the output embedding can be of your choice. just like the hidden layer output dimensions of mlp"
358,training on rnn and gru too slow even using gpu.do we need to iterate through entire training dataset in one epoch? it takes about 1 hr to run 10 epoch for rnn which i directly follow the tutorial in the pdf and add a optimizer. but in the tutorial seems only train on one document for one epoch. even worse for gru which i wait 20 minutes and does not finish one epoch. is there suggestion to solve this problem? by the way i choose hidden size as 30
362,does anyone have the same problem that rnn and gru loss does not decrease?.i trained for 100 epoch and it keeps at a very high value same as beginning. stuck here and debug for hours and did not see a difference. does anyone have the same problem? any suggestion?
362,maybe change the learning rate and see if it helps.
594,how to handle oovs?.what are the different ways to handle oov (out of vocabulary) words?
594,"we talked about that in the ""subword representation, contextualized representation"" lecture. please refer to those slides."
570,"knn classification k = 1.is there a situation that using k = 1 is acceptable for knn. if k=1 gives the highest model performance on an “unseen” dev set, does it make sense to use k=1?"
570,"in that case you do not really need to think about using an ml model. if your real world data maps 1:1 to your train data, you might as well write a sql(elasticsearch would be better) query with training data as your database."
544,"inter-rather agreement vs inter-annotator agreement.hello,are these two terms the same thing — just iaa being a bit more specified for annotation? otherwise i have seen them be used interchangeably in many places, and are measures with the same method’s, like cohen’s kappa."
544,same
487,"tensorflow or pytorch?.hello, tensorflow and pytorch seem to be the most popular ml libraries today. is there any recommendation on what is more suitable for nlp tasks? in terms of learning curve, industry-relevance, ease of deployment, etc.? thanks."
487,"historically, pytorch had a better performance on recursive networks (e.g. lstm) that were dominant before transformers, but they are both pretty good now. i personally use pytorch as it feels more natural to me but tf2.0 is also pretty mature and powerful once you get into the mindset. i cannot comment about industry usage. so just pick one."
382,"pos tagging: preprocessing.is preprocessing generally done with pos tagging? such as removing stop words, removing punctuations, or stemming to name a few.thanks"
382,"i do not think we are supposed to do this because stopwords too have a part of speech associated with them. so we are needed to predict that. also, these stopwords and predictions define the probability of certain pos, if you remove them you model will not take this into account. certain pos can have more probability to end or start the sentence than others. removing stopwords, punctuation etc will ignore this. i think stemming too should not be done as it can change the pos tag. please correct me if i am wrong."
268,"does encoder (decoder) imply a specific model or just a general speaking of sentence representation?.in the class, prof. chen talked about bi-encoder & cross-encoder, and i am confused that if the encoder is a specific model or it could be any kind of sentence representation method? for example, the output of lstm, or cnn, or self-attention? since the key word ""encoder"" (and ""decoder"") was mentioned several times during the lecture, i want to figure out what it really means. thanks in advance!"
268,"any kind of sequence encoders: cnns, lstms, self-attention encoders or transformers (multi-head attention). note that cross-encoder architectures usually use attention encoders nowadays."
255,"vanishing gradient.do all models that use gradient descent, have a vanishing gradient problem?"
255,"it depends on the depth of the neural networks. consider a one-layer nn, there is no vanishing gradient problem. however, for a very depth model (consider a 100-layer fully connected nn), it is higher chance that there is a vanishing gradient problem."
251,query regarding rnns.do rnns have a fixed vocabulary limitation ?
251,"depends. an rnn is simply a sequence model. if your input can handle oov tokens (eg. passing fasttext embs to rnn), then then the answer is no. if you are simply using something like word2vec or one hot then yes."
243,"convex optimization problem.hello,can you explain what is meant by saying an optimization problem is convex? and how can we actually determine whether an optimization problem is convex?"
243,"convex optimization means you are minimize a convex function. if you minimize a convex function, each local minima is global minima. for example, if you want to use a quadratic function to fit a series of data points, choosing the parameter of the quadratic function is a convex optimization problem."
103,naive bayes.we are adding logarithmic probabilities instead of multiplying conditional probabilities because it can result in floating-point underflow. what is floating-point underflow? any example?
103,"since probabilities lie between 0 and 1, multiplying a lot of them can result in very small numbers that the data type cannot represent. arithmetic underflow can occur when the true result of a floating point operation is smaller in magnitude (that is, closer to zero) than the smallest value representable as a normal floating point number in the target datatype.[1] arithmetic underflow - wikipedia floating point - what is the range of values a float can have in python? - stack overflow in naive bayes, i think it would affect the comparison, as probabilities that are actually different would end up being 0 and look equal."
54,"logistic regression robustness to correlated features.in the logistic regression reading material, under the section 5.2.4 (choosing a classifier), it says logistic regression is more robust to correlated features : ""if two features f1 and f2 are perfectly correlated, regression will simply assign part of the weight to w1 and part to w2"" . is not the test for correlated features (test for multi-collinearity) an important step that we check before building the model ? as, the independence of two features is very important criteria in finding the partial gradient ??"
54,"generally yes. one should get rid of correlated features before fitting the model. however, it is always preferred if your model is robust to these without expert intervention.in addition, it is not always feasible to remove correlated features, e.g. due to limitations of the data or bias in the data. no training data is perfect, especially in nlp."
18,emojis.are emojis considered to be part of nlp? it feels like it would be a relevant factor — such as for sentiment analysis.
18,"not sure if i understand the question. so please correct me if i missed it. is your question ""can processing emojis be done as part of nlp? especially for your assignment."" then yes it can be. just think of what you want to do with them as i have seen several research papers and projects on the topic. however, if you are asking ""are emojis a natural language?"". here is my opinion and others can join the discussion as well. i believe yes, emojis are a natural language. although they are manufactured originally, they have evolved naturally in human communities by repetition without conscious planning way beyond their original design. i found this interesting article on the topic:"
