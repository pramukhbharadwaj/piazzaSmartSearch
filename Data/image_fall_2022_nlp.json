[
    {
        "history_size": 1,
        "folders": [
            "quizzes"
        ],
        "nr": 627,
        "data": {
            "embed_links": []
        },
        "created": "2022-10-18T21:24:44Z",
        "bucket_order": 3,
        "no_answer_followup": 1,
        "change_log": [
            {
                "anon": "stud",
                "data": "l9eprghdsad6rh",
                "v": "all",
                "type": "create",
                "when": "2022-10-18T21:24:44Z",
                "uid_a": "a_0"
            },
            {
                "anon": "no",
                "uid": "kspcd36luqb4yn",
                "data": "l9epvfsyibuwa",
                "to": "l9eprgh9qka6rf",
                "type": "s_answer",
                "when": "2022-10-18T21:27:50Z"
            },
            {
                "anon": "no",
                "uid": "kspcd36luqb4yn",
                "data": "l9eq5b1rizk4rn",
                "type": "s_answer_update",
                "when": "2022-10-18T21:35:31Z"
            },
            {
                "anon": "no",
                "uid": "kspcd36luqb4yn",
                "data": "l9eq8tiqojg5l5",
                "type": "s_answer_update",
                "when": "2022-10-18T21:38:15Z"
            },
            {
                "anon": "no",
                "uid": "kspcd36luqb4yn",
                "data": "l9eqq4w48np6u5",
                "type": "s_answer_update",
                "when": "2022-10-18T21:51:42Z"
            },
            {
                "anon": "no",
                "uid": "kstgcok9xy24tk",
                "to": "l9eprgh9qka6rf",
                "type": "followup",
                "when": "2022-10-19T01:40:08Z",
                "cid": "l9eyvw16v5o4q7"
            }
        ],
        "bucket_name": "Today",
        "history": [
            {
                "anon": "stud",
                "uid_a": "a_0",
                "subject": "Quiz 8",
                "created": "2022-10-18T21:24:44Z",
                "content": "Question 8 0 out of 10 points\n\nOne of the difference between rule-based and transfer-based machine translation is that transfer-based machine translation's rules on semantic structure is more\ngeneralizable.\n\nSelected Answer: \u20ac3 True\nAnswers: True\n@ False\n\nQuestion 4 0 out of 10 points\n\nIn IBM machine translation model, if there are k words in the source sentence and v words in target sentence in a pair in parallel corpus, how many possible alignments\n13] exists in this pair. (a*b means a to the power of b)\n\nSelected Answer: \u00a2 (v+1)4k\nAnswers: k4v\n\n(v+41)4k\n\n@ (k+1)4v\n\nv\u2018k\n\nPreriwll Weuwviu vi Lf = bllitreliaeinte Prweieie\nAnalignment a is {a1,...@m} ,where a; \u20ac {0...1}\nHence there are (/ + 1)\u201d possible alignments\n\nm-.. t{re nr arrrrFri\n",
                "content_orig": "<p><img src=\"/redirect/s3?bucket=uploads&amp;prefix=paste%2Fksgx8cptzdp6tz%2Feb43d1581e169f3ad95694551d351eee13665e2bdc2f83aa75e343b4a9b2896a%2Fimage.png\" alt=\"image.pngNaN\" width=\"1421\" height=\"250\" /></p>\n<p>The correct answer for this question must be True, since transfer based model is more generalizable than rule based.</p>\n<p></p>\n<p>Also, for question 4</p>\n<p></p>\n<p><img src=\"/redirect/s3?bucket=uploads&amp;prefix=paste%2Fksgx8cptzdp6tz%2F32e75c2cdd164d642d972139c47feec6d915c54c2ff9cfc7bf3e135175c662c8%2Fimage.png\" alt=\"image.pngNaN\" /></p>\n<p></p>\n<p><img src=\"/redirect/s3?bucket=uploads&amp;prefix=paste%2Fksgx8cptzdp6tz%2Fac0b7443f53ec74abef56e8f4b56d9639fdf821f80f15df195bf4d387db61227%2Fimage.png\" alt=\"image.pngNaN\" /></p>\n<p></p>\n<p>as per this, ans should be (v&#43;1)^k.</p>"
            }
        ],
        "type": "question",
        "tags": [
            "quizzes",
            "student"
        ],
        "tag_good": [
            {
                "role": "student",
                "name": "Priya Mane",
                "endorser": {},
                "admin": false,
                "photo": null,
                "id": "ksgx8cptzdp6tz",
                "photo_url": null,
                "published": true,
                "us": false,
                "facebook_id": null
            },
            {
                "role": "student",
                "name": "Devyan Biswas",
                "endorser": {
                    "l7102doc7aa3ob": 1661693544
                },
                "admin": false,
                "photo": null,
                "id": "ksggzt30igi3w0",
                "photo_url": null,
                "published": true,
                "us": false,
                "facebook_id": null
            },
            {
                "role": "student",
                "name": "Aditya Dave",
                "endorser": {
                    "kyeg608so5gf5": 1644813528,
                    "global": 1645489099
                },
                "admin": false,
                "photo": null,
                "id": "ksav92ebzf3cj",
                "photo_url": null,
                "published": true,
                "us": false,
                "facebook_id": null
            },
            {
                "role": "student",
                "name": "Vashi Dhankar",
                "endorser": {},
                "admin": false,
                "photo": null,
                "id": "kjw2zl256e44nz",
                "photo_url": null,
                "published": true,
                "us": false,
                "facebook_id": null
            },
            {
                "role": "student",
                "name": "Joohan Lee",
                "endorser": {},
                "admin": false,
                "photo": null,
                "id": "ky6gkg81v702xo",
                "photo_url": null,
                "published": true,
                "us": false,
                "facebook_id": null
            },
            {
                "role": "student",
                "name": "Parth Vipul Shah",
                "endorser": {},
                "admin": false,
                "photo": null,
                "id": "ky843ck9nvj1w2",
                "photo_url": null,
                "published": true,
                "us": false,
                "facebook_id": null
            }
        ],
        "unique_views": 138,
        "children": [
            {
                "history_size": 4,
                "folders": [],
                "data": {
                    "embed_links": []
                },
                "created": "2022-10-18T21:27:50Z",
                "bucket_order": 3,
                "tag_endorse": [
                    {
                        "role": "ta",
                        "name": "Yuliang Cai",
                        "endorser": {},
                        "admin": true,
                        "photo": null,
                        "id": "l710c50jh211c3",
                        "photo_url": null,
                        "us": false,
                        "facebook_id": null
                    },
                    {
                        "role": "student",
                        "name": "Kaustubh Rai",
                        "endorser": {},
                        "admin": false,
                        "photo": null,
                        "id": "ky843den1e11yf",
                        "photo_url": null,
                        "published": true,
                        "us": false,
                        "facebook_id": null
                    }
                ],
                "bucket_name": "Today",
                "history": [
                    {
                        "anon": "no",
                        "uid": "kspcd36luqb4yn",
                        "subject": "",
                        "created": "2022-10-18T21:51:42Z",
                        "content": "<p>Completely agree with you on transfer-based Machine Translation problem.\u00a0</p>\n<p></p>\n<p>For the IBM model problem, this is my understanding:</p>\n<p>In the ppt you screenshot, <strong>l </strong>is the length of the <strong>source </strong>and <strong>m </strong>is the length of the <strong>target</strong>, therefore (<strong>l</strong>&#43;1)^<strong>m </strong>possible alignments.</p>\n<p>- For clarification: The slide states, &#34;[a]n alignment map determines which English word each French word originated from.&#34; Meaning that the French word (target) came from the English word (source). (<em>I honestly may be confusing this though.</em>)</p>\n<p></p>\n<p>In the quiz, <strong>k </strong>is the length of the <strong>source </strong>and <strong>v </strong>is the length of the <strong>target</strong>, therefore (<strong>k</strong>&#43;1)^<strong>v </strong>possible alignments..\u00a0</p>"
                    },
                    {
                        "anon": "no",
                        "uid": "kspcd36luqb4yn",
                        "subject": "",
                        "created": "2022-10-18T21:38:15Z",
                        "content": "<p>Completely agree with you on transfer-based Machine Translation problem.\u00a0</p>\n<p></p>\n<p>For the IBM model problem, this is my understanding:</p>\n<p>In the ppt you screenshot, <strong>l </strong>is the length of the <strong>source </strong>and <strong>m </strong>is the length of the <strong>target</strong>, therefore (<strong>l</strong>&#43;1)^<strong>m </strong>possible alignments.</p>\n<p>- For clarification: The slide states, &#34;[a]n alignment map determines which English word each French word originated from.&#34; Meaning that the French word (target) came from the English word (source). (I honestly may be confusing this though. )</p>\n<p></p>\n<p>In the quiz, <strong>k </strong>is the length of the <strong>source </strong>and <strong>v </strong>is the length of the <strong>target</strong>, therefore (<strong>k</strong>&#43;1)^<strong>v </strong>possible alignments..\u00a0</p>"
                    },
                    {
                        "anon": "no",
                        "uid": "kspcd36luqb4yn",
                        "subject": "",
                        "created": "2022-10-18T21:35:31Z",
                        "content": "<p></p>\n<p>From my understanding, in the ppt you screenshot, <strong>l </strong>is the length of the <strong>source </strong>and <strong>m </strong>is the length of the <strong>target</strong>, therefore (<strong>l</strong>&#43;1)^<strong>m </strong>possible alignments.</p>\n<p>- For clarification: The slide states, &#34;[a]n alignment map determines which English word each French word originated from.&#34; Meaning that the French word (target) came from the English word (source). (<em>I honestly may be confusing this though.</em>)</p>\n<p></p>\n<p>In the quiz, <strong>k </strong>is the length of the <strong>source </strong>and <strong>v </strong>is the length of the <strong>target</strong>, therefore (<strong>k</strong>&#43;1)^<strong>v </strong>possible alignments..\u00a0</p>"
                    },
                    {
                        "anon": "no",
                        "uid": "kspcd36luqb4yn",
                        "subject": "",
                        "created": "2022-10-18T21:27:50Z",
                        "content": "<p></p>\n<p>From my understanding, in the ppt you screenshot, <strong>l </strong>is the length of the <strong>source </strong>and <strong>m </strong>is the length of the <strong>target</strong>, therefore (<strong>l</strong>&#43;1)^<strong>m </strong>possible alignments.</p>\n<p></p>\n<p>In the quiz, <strong>k </strong>is the length of the <strong>source </strong>and <strong>v </strong>is the length of the <strong>target</strong>, therefore (<strong>k</strong>&#43;1)^<strong>v </strong>possible alignments..\u00a0</p>"
                    }
                ],
                "type": "s_answer",
                "tag_endorse_arr": [
                    "l710c50jh211c3",
                    "ky843den1e11yf"
                ],
                "children": [],
                "id": "l9epvfsvkugw9",
                "config": {
                    "editor": "rte"
                },
                "is_tag_endorse": false
            },
            {
                "anon": "no",
                "folders": [],
                "data": {
                    "embed_links": null
                },
                "no_upvotes": 0,
                "subject": "<p>For question 8, the question is talking about the semantic structure and not the syntactic structure. Hence, this will be False!</p>",
                "created": "2022-10-19T01:40:08Z",
                "bucket_order": 4,
                "bucket_name": "Yesterday",
                "type": "followup",
                "tag_good": [],
                "uid": "kstgcok9xy24tk",
                "children": [],
                "tag_good_arr": [],
                "no_answer": 1,
                "id": "l9eyvw16v5o4q7",
                "d-bucket": "Yesterday",
                "updated": "2022-10-19T01:44:09Z",
                "config": {
                    "editor": "rte"
                }
            }
        ],
        "tag_good_arr": [
            "ksgx8cptzdp6tz",
            "ksggzt30igi3w0",
            "ksav92ebzf3cj",
            "kjw2zl256e44nz",
            "ky6gkg81v702xo",
            "ky843ck9nvj1w2"
        ],
        "no_answer": 0,
        "id": "l9eprgh9qka6rf",
        "config": {
            "editor": "rte",
            "has_emails_sent": 1
        },
        "status": "active",
        "drafts": null,
        "request_instructor": 0,
        "request_instructor_me": false,
        "bookmarked": 3,
        "num_favorites": 2,
        "my_favorite": false,
        "is_bookmarked": false,
        "is_tag_good": false,
        "q_edits": [],
        "i_edits": [],
        "s_edits": [],
        "t": 1669603429290,
        "default_anonymity": "no",
        "question_link": "https://piazza.com/class/l7102doc7aa3ob/post/627"
    },
    {
        "history_size": 1,
        "folders": [
            "coding3"
        ],
        "nr": 529,
        "data": {
            "embed_links": []
        },
        "created": "2022-10-06T17:33:35Z",
        "bucket_order": 3,
        "no_answer_followup": 0,
        "change_log": [
            {
                "anon": "stud",
                "data": "l8xc7ylph333xr",
                "v": "all",
                "type": "create",
                "when": "2022-10-06T17:33:35Z",
                "uid_a": "a_0"
            },
            {
                "anon": "no",
                "uid": "ksnna3fv70v6hr",
                "data": "l8xcbx1dw3y31",
                "to": "l8xc7ylm7nn3xq",
                "type": "s_answer",
                "when": "2022-10-06T17:36:39Z"
            },
            {
                "anon": "no",
                "uid": "ixm5rfb79wz6t",
                "data": "l8xiyb5pa53xj",
                "to": "l8xc7ylm7nn3xq",
                "type": "i_answer",
                "when": "2022-10-06T20:42:02Z"
            }
        ],
        "bucket_name": "Today",
        "history": [
            {
                "anon": "stud",
                "uid_a": "a_0",
                "subject": "Dealing with unknown words in test data.",
                "created": "2022-10-06T17:33:35Z",
                "content": "= Smoothing and unseen words and transitions. You should implement some method to handle unknown vocabulary and unseen transitions in the test data, otherwise your\nprograms won't work.\n\n= Unseen words: The test data may contain words that have never been encountered in the training data: these will have an emission probability of zero for all tags.\n= Unseen transitions: The test data may contain two adjacent unambiguous words (that is, words that can only have one part-of-speech tag), but the transition between\n\nthese tags was never seen in the training data, so it has a probability of zero; in this case the Viterbi algorithm will have no way to proceed.\n\nThe reference solution will use add-one smoothing on the transition probabilities and no smoothing on the emission probabilities. For unknown tokens in the test data, it will\n\nignore the emission probabilities and use the transition iiti and also limit the tag inventory to only the tags with a large associated vocabulary (open-class\nitems). You may use more sophisticated methods which you implement yourselves.\n",
                "content_orig": "<p><img src=\"/redirect/s3?bucket=uploads&amp;prefix=paste%2Fksgx8cptzdp6tz%2F164cc5dbcafb5239812bd72077a687511cbc29c488e35a90e0b4d81d38353eb0%2Fimage.png\" alt=\"image.pngNaN\" /></p>\n<p>does &#34;ignoring emission prob&#34; mean considering it 1?</p>\n<p></p>\n<p>Eg.</p>\n<p>because for any tag, the prob of reaching it from a prev_tag is:\u00a0 prev_prob*emission_prob*transition_prob.</p>\n<p>which means the final prob will be\u00a0 prev_prob*transition_prob</p>\n<p>Does ignoring mean considering emission prob as 1?</p>\n<p></p>\n<p>Thank you!</p>"
            }
        ],
        "type": "question",
        "tags": [
            "coding3",
            "student"
        ],
        "tag_good": [],
        "unique_views": 158,
        "children": [
            {
                "history_size": 1,
                "folders": [],
                "data": {
                    "embed_links": []
                },
                "created": "2022-10-06T17:36:39Z",
                "bucket_order": 3,
                "tag_endorse": [
                    {
                        "role": "student",
                        "name": "Priya Mane",
                        "endorser": {},
                        "admin": false,
                        "photo": null,
                        "id": "ksgx8cptzdp6tz",
                        "photo_url": null,
                        "published": true,
                        "us": false,
                        "facebook_id": null
                    },
                    {
                        "role": "student",
                        "name": "yuhangjin",
                        "endorser": {},
                        "admin": false,
                        "photo": null,
                        "id": "ky9hzzsk9wi3x6",
                        "photo_url": null,
                        "published": true,
                        "us": false,
                        "facebook_id": null
                    },
                    {
                        "role": "student",
                        "name": "Fan Chung Hung",
                        "endorser": {},
                        "admin": false,
                        "photo": null,
                        "id": "ky7vx132ye45",
                        "photo_url": null,
                        "published": true,
                        "us": false,
                        "facebook_id": null
                    }
                ],
                "bucket_name": "Today",
                "history": [
                    {
                        "anon": "no",
                        "uid": "ksnna3fv70v6hr",
                        "subject": "",
                        "created": "2022-10-06T17:36:39Z",
                        "content": "<p>Yes, since any other value of the emissions probability would lead to some sort of influence of the overall state probability. For unseen words, I just set emission probability to 1 to account for that.\u00a0</p>"
                    }
                ],
                "type": "s_answer",
                "tag_endorse_arr": [
                    "ksgx8cptzdp6tz",
                    "ky9hzzsk9wi3x6",
                    "ky7vx132ye45"
                ],
                "children": [],
                "id": "l8xcbx19f092z",
                "config": {
                    "editor": "rte"
                },
                "is_tag_endorse": false
            },
            {
                "history_size": 1,
                "folders": [],
                "data": {
                    "embed_links": []
                },
                "created": "2022-10-06T20:42:02Z",
                "bucket_order": 3,
                "tag_endorse": [],
                "bucket_name": "Today",
                "history": [
                    {
                        "anon": "no",
                        "uid": "ixm5rfb79wz6t",
                        "subject": "",
                        "created": "2022-10-06T20:42:02Z",
                        "content": "Ignoring the emission is not the same as assuming the probability is one. Mathematically, of course, multiplying a number by one is the same as leaving the number alone. But conceptually these are different. Assuming all tags have a probability of 1 for emitting any unseen word is incoherent. Instead, what we&#39;re doing is an algorithmic change: whereas for seen words, the Viterbi decoding lattice (or matrix) registers the joint probability of the incoming path, latest transition, and emission, for unseen words the lattice registers the joint probability of the incoming path and latest transition. Since it does the same for the entire column, the numbers are comparable.\n\nIf you wanted to use an emission probability for unseen items, you would need to include that in your model. For example, you could divide your training data in order to estimate for each state the probability that it would generate an unseen token, and then build these estimates into the model."
                    }
                ],
                "type": "i_answer",
                "tag_endorse_arr": [],
                "children": [],
                "id": "l8xiyb5ovbxxi",
                "config": {
                    "editor": "plain"
                },
                "is_tag_endorse": false
            }
        ],
        "tag_good_arr": [],
        "no_answer": 0,
        "id": "l8xc7ylm7nn3xq",
        "config": {
            "editor": "rte",
            "has_emails_sent": 1
        },
        "status": "active",
        "drafts": null,
        "request_instructor": 0,
        "request_instructor_me": false,
        "bookmarked": 4,
        "num_favorites": 2,
        "my_favorite": false,
        "is_bookmarked": false,
        "is_tag_good": false,
        "q_edits": [],
        "i_edits": [],
        "s_edits": [],
        "t": 1669603431394,
        "default_anonymity": "no",
        "question_link": "https://piazza.com/class/l7102doc7aa3ob/post/529"
    }
]