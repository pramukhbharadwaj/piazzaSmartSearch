{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "eYUddGxnGSd5"
      },
      "outputs": [],
      "source": [
        "!pip --q install sentence_transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "72aHZStmGckG"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load cleaned corpus data\n",
        "df=pd.read_csv(\"Data/clean_data.csv\")\n",
        "\n",
        "#Load language model\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Embed all documents\n",
        "corpus_embeddings = embedder.encode(df['Sentence'], convert_to_tensor=True)\n",
        "\n",
        "# Query sentences:\n",
        "#call data cleaning....\n",
        "queries = ['What kind of model is used in brill tagger ','why is logistic regression more robust to correlated features', 'How to tackle out of vocabulary']\n"
      ],
      "metadata": {
        "id": "YqRwJhlSMhub"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
        "top_k = 5\n",
        "for query in queries:\n",
        "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
        "\n",
        "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
        "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
        "    top_results = torch.topk(cos_scores, k=top_k)\n",
        "\n",
        "    print(\"\\n\\n======================\\n\\n\")\n",
        "    print(\"Query:\", query)\n",
        "    print(\"\\nTop most similar posts:\")\n",
        "    posts_seen=set()\n",
        "    for score, idx in zip(top_results[0], top_results[1]):\n",
        "      #Skip, if this post is already recommended\n",
        "      post=df.iloc[idx.item()]['Post']\n",
        "      # if post not in posts_seen:\n",
        "      #   posts_seen.add(post)\n",
        "      print(\"\\nPost: {}\\nDocument: {}\\nScore: {:.4f}\".format(post,df.iloc[idx.item()]['Sentence'],score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBuHgKpeXqf4",
        "outputId": "18951889-c23a-4d2f-a144-c7a0fc4a7cec"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: What kind of model is used in brill tagger \n",
            "\n",
            "Top most similar posts:\n",
            "\n",
            "Post: 506\n",
            "Document: is brill tagger a discriminative model or generative model?.\n",
            "Score: 0.7269\n",
            "\n",
            "Post: 506\n",
            "Document: neither. the model does not output probabilities of tags or sequences of tags.\n",
            "Score: 0.4479\n",
            "\n",
            "Post: 528\n",
            "Document: it is a tradeoff of runtime and accuracy. my best recommendation is to try it both ways: implement your model with and without smoothing on the emissions, measure runtime and accuracy for both options, and decide if the tradeoff is worthwhile. looking at the theory, smoothing the emission probabilities is expected to cause a substantial increase in runtime, because now you are calculating paths into every tag for every word, instead of throwing away most of the paths, it also has the potential for an improvement in accuracy, since you may run into a situation where context strongly suggests a tag but the current word was only seen with other tags. however, the improvement in accuracy is likely to be fairly modest, because if a word was seen with some tags but not others, then either the word is rare or the other tags are rare for that word.\n",
            "Score: 0.2760\n",
            "\n",
            "Post: 511\n",
            "Document: yes and yes. i am not sure if the data we are working with will have such cases. but if we do, then you are correct. it can happen that we do not see a tag in train dataset because of which we assign words belonging to that tag incorrectly when tagging test dataset. no there is no known list of part of speech tags. we should be using and learning model from the ones that are present in training data.\n",
            "Score: 0.2696\n",
            "\n",
            "Post: 79\n",
            "Document: lemmatization based on pos tag.the ntlk lemmatizer will not lemmatize \"wearing\" to \"wear\" unless explicit mentioned to considered it as a word, because the default tag is \"noun\" . is this lemmatization acceptable?\n",
            "Score: 0.2523\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: why is logistic regression more robust to correlated features\n",
            "\n",
            "Top most similar posts:\n",
            "\n",
            "Post: 54\n",
            "Document: logistic regression robustness to correlated features.in the logistic regression reading material, under the section 5.2.4 (choosing a classifier), it says logistic regression is more robust to correlated features : \"if two features f1 and f2 are perfectly correlated, regression will simply assign part of the weight to w1 and part to w2\" . is not the test for correlated features (test for multi-collinearity) an important step that we check before building the model ? as, the independence of two features is very important criteria in finding the partial gradient ??\n",
            "Score: 0.6957\n",
            "\n",
            "Post: 54\n",
            "Document: generally yes. one should get rid of correlated features before fitting the model. however, it is always preferred if your model is robust to these without expert intervention.in addition, it is not always feasible to remove correlated features, e.g. due to limitations of the data or bias in the data. no training data is perfect, especially in nlp.\n",
            "Score: 0.4026\n",
            "\n",
            "Post: 22\n",
            "Document: it suppresses large values to avoid making some feature values that correspond to infrequent ones too large. in practice, this has impression been found to be helpful.\n",
            "Score: 0.3096\n",
            "\n",
            "Post: 20\n",
            "Document: in this case, the feature might not be a good representation of the document. you can expand the vocabulary to make the representation less sparse, at the cost of increasing the dimension of the vector. bow is a simple model, it suffers from the problems like the one you mentioned inherently. it is useful in some simple applications, when you do not have to deal with the outliers.\n",
            "Score: 0.2905\n",
            "\n",
            "Post: 20\n",
            "Document: bag of words.how is a model which encounters a vector space representation of word vs document which is extremely sparse affected? what can be done to handle the negative effects on the model?\n",
            "Score: 0.1853\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: How to tackle out of vocabulary\n",
            "\n",
            "Top most similar posts:\n",
            "\n",
            "Post: 594\n",
            "Document: how to handle oovs?.what are the different ways to handle oov (out of vocabulary) words?\n",
            "Score: 0.5596\n",
            "\n",
            "Post: 608\n",
            "Document: ignoring an unknown word is probably the simplest solution. alternatively, you can devise features that could be triggered by both known and unknown words.\n",
            "Score: 0.4365\n",
            "\n",
            "Post: 362\n",
            "Document: maybe change the learning rate and see if it helps.\n",
            "Score: 0.4213\n",
            "\n",
            "Post: 589\n",
            "Document: using words as features, each word can be considered a feature, and the count of the word is the value. so a review like \"the omni is the best hotel in the world\" will have the following feature values: {'best': 1, 'hotel': 1, 'in': 1, 'is': 1, 'omni': 1, 'the': 3, 'world': 1} if you want to think of this as a fixed-length vector, then the length would be the entire vocabulary, and most values will be zero. notice how in the feature structure above, i silently changed 'the' to 'the', but kept 'omni' without change. this would be difficult to do programmatically, but this is one of the things to experiment with: is it helpful to change case? mess around with suffixes? do something with punctuation? is it helpful to develop features that are not words? for example, use word bigrams as features? are there any words that should be excluded from being used as features? i recommend starting with very simple features (say, just words), in order to get the basic logic of the perceptron working, once you have that, you can experiment to see how changes to features affect performance of the perceptron. no, word2vec is not allowed for this assignment.\n",
            "Score: 0.4039\n",
            "\n",
            "Post: 20\n",
            "Document: bag of words.how is a model which encounters a vector space representation of word vs document which is extremely sparse affected? what can be done to handle the negative effects on the model?\n",
            "Score: 0.3815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JYWm41VnVmbD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}