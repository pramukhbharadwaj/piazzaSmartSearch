{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hXWzV4yaYg99",
    "outputId": "e2dd68bd-c260-40a1-b1d9-211fc25efbf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting pyahocorasick\n",
      "  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
      "\u001b[K     |████████████████████████████████| 106 kB 2.8 MB/s \n",
      "\u001b[?25hCollecting anyascii\n",
      "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
      "\u001b[K     |████████████████████████████████| 287 kB 21.0 MB/s \n",
      "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.1 contractions-0.1.73 pyahocorasick-1.4.4 textsearch-0.0.24\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_dGZ64ZkYH0o"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "1XD_-Yb1Yl9W"
   },
   "outputs": [],
   "source": [
    "def isEndorsedByStaff(endorsements):\n",
    "    for endorsement in endorsements:\n",
    "        if 'role' in endorsement and ('professor' in endorsement['role'] or 'instructor' in endorsement['role'] or 'ta' in endorsement['role']):\n",
    "            return True\n",
    "\n",
    "def checkValidAnswer(post):\n",
    "    return ('i_answer' in post['type']) or ('tag_endorse' in post and isEndorsedByStaff(post['tag_endorse']))\n",
    "\n",
    "def getAnswerList(post):\n",
    "    answerList = []\n",
    "    if('children' in post):\n",
    "        postAnswers = post['children']\n",
    "        for postAnswer in postAnswers:\n",
    "            #whoAnswered = ''\n",
    "            answer = ''\n",
    "            if 'type' in postAnswer and checkValidAnswer(postAnswer) and 'history' in postAnswer and 'subject' not in postAnswer['history']:\n",
    "                #whoAnswered = postAnswer['type']\n",
    "                #last_modified = postAnswer['history'][len(postAnswer['history']) - 1]\n",
    "                last_modified = getLastModified(postAnswer)\n",
    "                answer = last_modified['content']\n",
    "                #answerToWhoAnswered = (answer, whoAnswered)\n",
    "                answerList.append(answer)\n",
    "                        \n",
    "    return answerList\n",
    "\n",
    "def getLastModified(post):\n",
    "    history = post['history']\n",
    "    last_modified_answer = history[0]\n",
    "    last_modified_datetime = dateutil.parser.parse(history[0]['created'])\n",
    "    for i in range(0, len(history)):\n",
    "        post_datetime = dateutil.parser.parse(history[i]['created'])\n",
    "        if(post_datetime > last_modified_datetime):\n",
    "            last_modified_datetime = post_date\n",
    "            last_modified_answer = history[i]\n",
    "            \n",
    "    return last_modified_answer\n",
    "\n",
    "def extractDataForEvaluation(filename):\n",
    "    with open(filename, 'r') as openfile:\n",
    "        input = json.load(openfile)\n",
    "        #print(input)\n",
    "        df = pd.DataFrame(columns = ['Post','Link','Question','AnswerList'])\n",
    "        for i in range(0, len(input)):\n",
    "            post = input[i]\n",
    "            if 'history' in post:\n",
    "                last_modified = getLastModified(post)\n",
    "                if 'subject' in last_modified and 'content' in last_modified:\n",
    "                    subject = last_modified['subject']\n",
    "                    content = last_modified['content']\n",
    "                    post_ID = post['nr']\n",
    "                    question_link = post['question_link']\n",
    "                    answerList = getAnswerList(input[i])\n",
    "                    df = df.append({'Post': post_ID, 'Link': question_link,'Question': subject + \".\" + content, 'AnswerList': answerList}, ignore_index = True)  \n",
    "        return df\n",
    "\n",
    "#Removing all contractions\n",
    "def perform_contractions(series):\n",
    "    series = series.apply(lambda x: contractions.fix(x))\n",
    "    return series\n",
    "\n",
    "def data_cleaning(data):\n",
    "\n",
    "  #Convert to Lowercase\n",
    "  data[\"Question\"] = data[\"Question\"].str.lower()\n",
    "\n",
    "  #Remove all HTML tags\n",
    "  data[\"Question\"] = data[\"Question\"].apply(lambda x: BeautifulSoup(str(x)).get_text())\n",
    "\n",
    "\n",
    "  #Remove all URLs\n",
    "  data[\"Question\"] = data[\"Question\"].apply(lambda x: re.sub(r'\\s*(https?://|www\\.)+\\S+(\\s+|$)', \" \", str(x), flags=re.UNICODE))\n",
    "\n",
    "  #Remove extra spaces\n",
    "  data[\"Question\"] = data[\"Question\"].apply(lambda x: re.sub(r\"\\s+\", \" \", str(x), flags=re.UNICODE).strip())\n",
    "\n",
    "\n",
    "  x = perform_contractions(data[\"Question\"])\n",
    "  data[\"Question\"] = x\n",
    "\n",
    "  #Removing uppercase letters which might be introduced after removing contractions\n",
    "  data[\"Question\"] = data[\"Question\"].str.lower()\n",
    "\n",
    "  data = data.reset_index(drop=True)\n",
    "\n",
    "  for i in range(0,len(data['AnswerList'])):\n",
    "    for j in range (0, len(data['AnswerList'][i])):\n",
    "      data['AnswerList'][i][j] = data['AnswerList'][i][j].lower()\n",
    "      data['AnswerList'][i][j] = BeautifulSoup(data['AnswerList'][i][j]).get_text()\n",
    "      data['AnswerList'][i][j] = re.sub(r'\\s*(https?://|www\\.)+\\S+(\\s+|$)', \" \", data['AnswerList'][i][j], flags=re.UNICODE)\n",
    "      data['AnswerList'][i][j] = re.sub(r\"\\s+\", \" \", data['AnswerList'][i][j], flags=re.UNICODE).strip()\n",
    "      data['AnswerList'][i][j] = contractions.fix(data['AnswerList'][i][j])\n",
    "\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "HMXeFRl2s51w"
   },
   "outputs": [],
   "source": [
    "evaluation_data = df=pd.concat([extractDataForEvaluation(\"Data/fall_2022_nlp.json\"), \n",
    "                    extractDataForEvaluation(\"Data/spring_2022_nlp.json\"),\n",
    "                  extractDataForEvaluation(\"Data/image_spring_2022_nlp.json\"),\n",
    "                  extractDataForEvaluation(\"Data/image_fall_2022_nlp.json\")])\n",
    "\n",
    "# Clean\n",
    "evaluation_data = data_cleaning(evaluation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yFNdzOy4t3lR",
    "outputId": "c02f0cb2-5d23-47e1-e4ad-86a3e04f97be"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/content/dataextracter.py:95: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 95 of the file /content/dataextracter.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  data[\"Sentence\"] = data[\"Sentence\"].apply(lambda x: BeautifulSoup(str(x)).get_text())\n"
     ]
    }
   ],
   "source": [
    "from dataextracter import *\n",
    "\n",
    "train_data=pd.concat([extractData(\"Data/fall_2022_nlp.json\"), \n",
    "                    extractData(\"Data/spring_2022_nlp.json\"),\n",
    "                  extractData(\"Data/image_spring_2022_nlp.json\"),\n",
    "                  extractData(\"Data/image_fall_2022_nlp.json\")])\n",
    "\n",
    "# Clean\n",
    "train_data = data_cleaning(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "olzMyp5Yuiq8"
   },
   "outputs": [],
   "source": [
    "!pip --q install sentence_transformers torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "L0oEa74AuZ8v"
   },
   "outputs": [],
   "source": [
    "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "#Load language model\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "corpus_embeddings = embedder.encode(train_data['Sentence'], convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qqtehfb-wwYf",
    "outputId": "d9545973-1ad3-445f-d204-539638d97af9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: quiz. 9 - neural language models use word-embedding models in their training..hi , when taught in class it was mentioned that a by product of neural language models was the word embeddings but i do not remember word embeddings to be involved in training. please can someone confirm this. thanks\n",
      "Expected Answer: yes, word embedding are not used to rain word2vec. we only use one-hot vectors and then learn word embeddings as the weights of the two-layer network.\n",
      "\n",
      "Top most similar posts:\n",
      "['no, that question is not about the two-layer network we used to learn word embeddings. that network itself never is used as a neural language modelf. the question is about neural language models that receive word embeddings as their input. in short, you need to pay attention to the network that the question is asking about.', 'so the question in quiz 9 should be false? the answer was true for the question: neural language models use word-embedding models in their training.', 'yes, word embedding are not used to rain word2vec. we only use one-hot vectors and then learn word embeddings as the weights of the two-layer network.', 'input to rnn.will the input of the rnn be a word embedding aka a vector of size (300,)? if yes then would not hidden state also have to be 300 ?']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: regarding random shuffling between epochs while training.i am getting better mean f1 scores for both vanilla and averaged perceptron without random shuffling the dataset between epochs while training. is shuffling between epochs mandatory while training? can i train the perceptron without shuffling the dataset?\n",
      "Expected Answer: not mandatory, but see @661\n",
      "\n",
      "Top most similar posts:\n",
      "['i also did not use shuffling. i was able to get above baseline f1', 'training on rnn and gru too slow even using gpu.do we need to iterate through entire training dataset in one epoch? it takes about 1 hr to run 10 epoch for rnn which i directly follow the tutorial in the pdf and add a optimizer. but in the tutorial seems only train on one document for one epoch. even worse for gru which i wait 20 minutes and does not finish one epoch. is there suggestion to solve this problem? by the way i choose hidden size as 30', 'gradient descent vs perceptron algorithms.i want to verify my understanding is correct. the perceptron algorithm describes the full process of assigning random weights, then viewing examples from training data, adjusting weights, etc. gradient descent is a specific optimization technique that can be used to adjust weights. so one could replace the \"vanilla\" method of adjusting weights from the perceptron algorithm with gradient descent?', 'does anyone have the same problem that rnn and gru loss does not decrease?.i trained for 100 epoch and it keeps at a very high value same as beginning. stuck here and debug for hours and did not see a difference. does anyone have the same problem? any suggestion?']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: feature extraction and classifying using one model.since i use tfidf for feature extraction, i need to keep the bag of words in the model for feature extraction in classifying right? does that mean i also need to store the idf for each word calculated in training and load it for classification? since the testing data need the same idf for the words in my bag of words. (unseen words will be discarded)\n",
      "Expected Answer: you could. another option would be to put the idf weights into your leaned weight vectors.\n",
      "\n",
      "Top most similar posts:\n",
      "['feature extraction and classifying using one model.since i use tfidf for feature extraction, i need to keep the bag of words in the model for feature extraction in classifying right? does that mean i also need to store the idf for each word calculated in training and load it for classification? since the testing data need the same idf for the words in my bag of words. (unseen words will be discarded)', 'tf-idf.should i run tf-idf and extract features treating entire dataset as corpus? or by doing it classwise??', 'it can be because you are using a non-sparse matrix format. tfidf features can be stored as sparse matrices.', 'how to use word tokens as features.in the overview of code assignment 4, it said that \"you may use the word tokens as features. \". professor said i can count words instead of using tf-idf. can anybody give me an example about how to transfer a review (i.e. a sequence of words/strings) into a vector (a sequence of numbers)?', 'how do we handle out of vocabulary words in the above approach? if our training set does not contain a word encountered during testing is there a preferred way to handle them? or will the testing set not have words not encountered in training?']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: feature vector for unseen words.as we are creating one-hot encoding to convert reviews into features, how can i handle any unseen words in the test dataset? should i skip the word? or is there any other better alternative?\n",
      "Expected Answer: ignoring an unknown word is probably the simplest solution. alternatively, you can devise features that could be triggered by both known and unknown words.\n",
      "\n",
      "Top most similar posts:\n",
      "['how do we handle out of vocabulary words in the above approach? if our training set does not contain a word encountered during testing is there a preferred way to handle them? or will the testing set not have words not encountered in training?', 'ignoring an unknown word is probably the simplest solution. alternatively, you can devise features that could be triggered by both known and unknown words.', 'using words as features, each word can be considered a feature, and the count of the word is the value. so a review like \"the omni is the best hotel in the world\" will have the following feature values: {\\'best\\': 1, \\'hotel\\': 1, \\'in\\': 1, \\'is\\': 1, \\'omni\\': 1, \\'the\\': 3, \\'world\\': 1} if you want to think of this as a fixed-length vector, then the length would be the entire vocabulary, and most values will be zero. notice how in the feature structure above, i silently changed \\'the\\' to \\'the\\', but kept \\'omni\\' without change. this would be difficult to do programmatically, but this is one of the things to experiment with: is it helpful to change case? mess around with suffixes? do something with punctuation? is it helpful to develop features that are not words? for example, use word bigrams as features? are there any words that should be excluded from being used as features? i recommend starting with very simple features (say, just words), in order to get the basic logic of the perceptron working, once you have that, you can experiment to see how changes to features affect performance of the perceptron. no, word2vec is not allowed for this assignment.', 'how to use word tokens as features.in the overview of code assignment 4, it said that \"you may use the word tokens as features. \". professor said i can count words instead of using tf-idf. can anybody give me an example about how to transfer a review (i.e. a sequence of words/strings) into a vector (a sequence of numbers)?']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: is a parallel corpus required for statistical machine translation?.please correct me if i am wrong but as per my understanding the noisy channel model for machine translation being a statistical model does require a parallel corpus, right?\n",
      "Expected Answer: the question about parallel corpus for the language model, not mt.\n",
      "\n",
      "Top most similar posts:\n",
      "['the question about parallel corpus for the language model, not mt.', \"quiz 8.question 8 0 out of 10 points one of the difference between rule-based and transfer-based machine translation is that transfer-based machine translation's rules on semantic structure is more generalizable. selected answer: €3 true answers: true @ false question 4 0 out of 10 points in ibm machine translation model, if there are k words in the source sentence and v words in target sentence in a pair in parallel corpus, how many possible alignments 13] exists in this pair. (a*b means a to the power of b) selected answer: ¢ (v+1)4k answers: k4v (v+41)4k @ (k+1)4v v‘k preriwll weuwviu vi lf = bllitreliaeinte prweieie analignment a is {a1,...@m} ,where a; € {0...1} hence there are (/ + 1)” possible alignments m-.. t{re nr arrrrfri\", 'depends. an rnn is simply a sequence model. if your input can handle oov tokens (eg. passing fasttext embs to rnn), then then the answer is no. if you are simply using something like word2vec or one hot then yes.', 'query regarding rnns.do rnns have a fixed vocabulary limitation ?']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: smoothing transition probability matrix.what is meant by smoothing each row in the transition probability matrix? if we are generally dividing by the number of times the previous tag is in the corpus, are we adding 1 to that number?\n",
      "Expected Answer: laplace smoothing was covered in the first week of class @490, and again last week (lecture 13 on october 4). smoothing each row just means treating each outgoing transition separately. that is, we perform smoothing on all the transitions from noun; separately, we perform smoothing on all the transitions from verb; and so on. because each of these is a separate probability distribution.\n",
      "\n",
      "Top most similar posts:\n",
      "['smoothing transition probability matrix.what is meant by smoothing each row in the transition probability matrix? if we are generally dividing by the number of times the previous tag is in the corpus, are we adding 1 to that number?', 'laplace smoothing was covered in the first week of class @490, and again last week (lecture 13 on october 4). smoothing each row just means treating each outgoing transition separately. that is, we perform smoothing on all the transitions from noun; separately, we perform smoothing on all the transitions from verb; and so on. because each of these is a separate probability distribution.', 'when there are many transitions whose probabilities are zero, it can also lead to dividing by zero. to avoid this, we add 1 to the numerator and some factor to the denominator. this is called laplace smoothing. we do not add 1 to that number. otherwise, the probability would become greater than 1', 'by initial probabilities, i assume you mean what jurafsky and martin call “an initial probability distribution over states” (top of page 9). this is the probability distribution of the first state in a sentence. to estimate the initial probability of state x, just take the number of sentences that start with x, and divide by the total number of sentences. do not forget to apply smoothing. an alternative approach is the algorithm presented in the written exercises that were distributed on blackboard. add a special initial state that is not associated with any observations, and assume that each sentence starts with a transition from that initial state to the first word-producing state. then the initial probability distribution from the previous paragraph becomes just another row in the transition matrix. the two approaches are mathematically equivalent.', \"smoothing the emissions matrix would sacrifice efficiency due to its size. edit to add further explanation: you only need to smooth the emission matrix for unseen words. this is because smoothing the entire emissions matrix will assign too much probability to unseen events. that is, smoothing will result in a words having probabilities for tags they were not associated with in the training. thus, sacrificing accuracy in addition to efficiency. smoothing is typically done to either the transition or the emissions matrix. transition is typically chosen because some probabilities need to be reserved as unseen. for example, what if a noun/verb pair were never seen in the training data? does that mean a noun/verb pair will never appear while testing? take the sentence 'dogs lick'. the model would not be able to assign a non-zero transition probability between dogs/noun -> lick/verb. therefore, the absence of smoothing would lead to the verb being mistagged, thus impacting accuracy.\"]\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: hmm model.do we have to use forward-backward algorithm to learn the model as shown in the reference?\n",
      "Expected Answer: the forward-backward algorithm is for unsupervised learning. you do not need to do that, because the exercise provides labeled training data.\n",
      "\n",
      "Top most similar posts:\n",
      "['the forward-backward algorithm is for unsupervised learning. you do not need to do that, because the exercise provides labeled training data.', 'i suppose you could do it that way, although, i think it makes more sense to smooth while creating the matrices in `hmmlearn.py`. especially because in the learn program you will have unbridled access to all training data and only need to deal with words and tags in the given corpora, without worrying about unseen tags/words. if you are still unsure, you can always try it both ways and see what yields the best results!', 'smoothing was covered in the first week of class, when talking about naive bayes. you can see descriptions of the process in jurafsky and martin, chapter 4, pages 5–6, and in manning, raghavan and schutze, chapter 13, page 260. smoothing for hmm works exactly the same way. you just need to make sure to separately smooth each row in the matrix, since each row represents an independent probability distribution. you are allowed to consult references. what is not allowed is consulting implementations (or texts that specifically describe an implementation).', 'by initial probabilities, i assume you mean what jurafsky and martin call “an initial probability distribution over states” (top of page 9). this is the probability distribution of the first state in a sentence. to estimate the initial probability of state x, just take the number of sentences that start with x, and divide by the total number of sentences. do not forget to apply smoothing. an alternative approach is the algorithm presented in the written exercises that were distributed on blackboard. add a special initial state that is not associated with any observations, and assume that each sentence starts with a transition from that initial state to the first word-producing state. then the initial probability distribution from the previous paragraph becomes just another row in the transition matrix. the two approaches are mathematically equivalent.']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: smoothing.instead of laplace smoothing, i am trying to use additive smoothing. still, i am not able to get good accuracy, leading not to be able to pinpoint the exact additive integer. is there any other way to do this?\n",
      "Expected Answer: sophisticated smoothing could lead to a small improvement, but if the accuracy you are getting is not in the general range of the baseline or reference, then you probably have a bug or error, and finding and fixing that would lead to more improvement than fiddling with the smoothing method\n",
      "\n",
      "Top most similar posts:\n",
      "['sophisticated smoothing could lead to a small improvement, but if the accuracy you are getting is not in the general range of the baseline or reference, then you probably have a bug or error, and finding and fixing that would lead to more improvement than fiddling with the smoothing method', 'laplace smoothing was covered in the first week of class @490, and again last week (lecture 13 on october 4). smoothing each row just means treating each outgoing transition separately. that is, we perform smoothing on all the transitions from noun; separately, we perform smoothing on all the transitions from verb; and so on. because each of these is a separate probability distribution.', 'smoothing was covered in the first week of class, when talking about naive bayes. you can see descriptions of the process in jurafsky and martin, chapter 4, pages 5–6, and in manning, raghavan and schutze, chapter 13, page 260. smoothing for hmm works exactly the same way. you just need to make sure to separately smooth each row in the matrix, since each row represents an independent probability distribution. you are allowed to consult references. what is not allowed is consulting implementations (or texts that specifically describe an implementation).', 'hmm smoothing.hi! i went through the algorithm in the references given to us and although there are tables showing the viterbi trellis for an example with and without smoothing, i do not particularly see how this smoothing is done. are we allowed to look for references on how smoothing is applied to the computation of transition probabilities or can you point some of the references we are allowed to use to this end? thanks!']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: how to use word tokens as features.in the overview of code assignment 4, it said that \"you may use the word tokens as features. \". professor said i can count words instead of using tf-idf. can anybody give me an example about how to transfer a review (i.e. a sequence of words/strings) into a vector (a sequence of numbers)?\n",
      "Expected Answer: using words as features, each word can be considered a feature, and the count of the word is the value. so a review like \"the omni is the best hotel in the world\" will have the following feature values: {'best': 1, 'hotel': 1, 'in': 1, 'is': 1, 'omni': 1, 'the': 3, 'world': 1} if you want to think of this as a fixed-length vector, then the length would be the entire vocabulary, and most values will be zero. notice how in the feature structure above, i silently changed 'the' to 'the', but kept 'omni' without change. this would be difficult to do programmatically, but this is one of the things to experiment with: is it helpful to change case? mess around with suffixes? do something with punctuation? is it helpful to develop features that are not words? for example, use word bigrams as features? are there any words that should be excluded from being used as features? i recommend starting with very simple features (say, just words), in order to get the basic logic of the perceptron working, once you have that, you can experiment to see how changes to features affect performance of the perceptron. no, word2vec is not allowed for this assignment.\n",
      "\n",
      "Top most similar posts:\n",
      "['using words as features, each word can be considered a feature, and the count of the word is the value. so a review like \"the omni is the best hotel in the world\" will have the following feature values: {\\'best\\': 1, \\'hotel\\': 1, \\'in\\': 1, \\'is\\': 1, \\'omni\\': 1, \\'the\\': 3, \\'world\\': 1} if you want to think of this as a fixed-length vector, then the length would be the entire vocabulary, and most values will be zero. notice how in the feature structure above, i silently changed \\'the\\' to \\'the\\', but kept \\'omni\\' without change. this would be difficult to do programmatically, but this is one of the things to experiment with: is it helpful to change case? mess around with suffixes? do something with punctuation? is it helpful to develop features that are not words? for example, use word bigrams as features? are there any words that should be excluded from being used as features? i recommend starting with very simple features (say, just words), in order to get the basic logic of the perceptron working, once you have that, you can experiment to see how changes to features affect performance of the perceptron. no, word2vec is not allowed for this assignment.', 'word2vec will transfer each word into a vector. in assignment 2, we transfer reviews/sentences into vectors in different ways (concatenation and average). i am not sure which one we should use. furthermore, training word2vec requires external library. i am not sure whether we are allowed to use it.', 'feature vector for unseen words.as we are creating one-hot encoding to convert reviews into features, how can i handle any unseen words in the test dataset? should i skip the word? or is there any other better alternative?', 'we talked about that in the \"subword representation, contextualized representation\" lecture. please refer to those slides.']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: dealing with unknown words in test data.. does \"ignoring emission prob\" mean considering it 1? eg. because for any tag, the prob of reaching it from a prev_tag is: prev_prob*emission_prob*transition_prob. which means the final prob will be prev_prob*transition_prob does ignoring mean considering emission prob as 1? thank you!\n",
      "Expected Answer: ignoring the emission is not the same as assuming the probability is one. mathematically, of course, multiplying a number by one is the same as leaving the number alone. but conceptually these are different. assuming all tags have a probability of 1 for emitting any unseen word is incoherent. instead, what we are doing is an algorithmic change: whereas for seen words, the viterbi decoding lattice (or matrix) registers the joint probability of the incoming path, latest transition, and emission, for unseen words the lattice registers the joint probability of the incoming path and latest transition. since it does the same for the entire column, the numbers are comparable. if you wanted to use an emission probability for unseen items, you would need to include that in your model. for example, you could divide your training data in order to estimate for each state the probability that it would generate an unseen token, and then build these estimates into the model.\n",
      "\n",
      "Top most similar posts:\n",
      "['ignoring the emission is not the same as assuming the probability is one. mathematically, of course, multiplying a number by one is the same as leaving the number alone. but conceptually these are different. assuming all tags have a probability of 1 for emitting any unseen word is incoherent. instead, what we are doing is an algorithmic change: whereas for seen words, the viterbi decoding lattice (or matrix) registers the joint probability of the incoming path, latest transition, and emission, for unseen words the lattice registers the joint probability of the incoming path and latest transition. since it does the same for the entire column, the numbers are comparable. if you wanted to use an emission probability for unseen items, you would need to include that in your model. for example, you could divide your training data in order to estimate for each state the probability that it would generate an unseen token, and then build these estimates into the model.', 'yes, since any other value of the emissions probability would lead to some sort of influence of the overall state probability. for unseen words, i just set emission probability to 1 to account for that.', 'it is a tradeoff of runtime and accuracy. my best recommendation is to try it both ways: implement your model with and without smoothing on the emissions, measure runtime and accuracy for both options, and decide if the tradeoff is worthwhile. looking at the theory, smoothing the emission probabilities is expected to cause a substantial increase in runtime, because now you are calculating paths into every tag for every word, instead of throwing away most of the paths, it also has the potential for an improvement in accuracy, since you may run into a situation where context strongly suggests a tag but the current word was only seen with other tags. however, the improvement in accuracy is likely to be fairly modest, because if a word was seen with some tags but not others, then either the word is rare or the other tags are rare for that word.', 'how do we handle out of vocabulary words in the above approach? if our training set does not contain a word encountered during testing is there a preferred way to handle them? or will the testing set not have words not encountered in training?']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: smoothing emission probability.why are we not smoothing emission probability?\n",
      "Expected Answer: it is a tradeoff of runtime and accuracy. my best recommendation is to try it both ways: implement your model with and without smoothing on the emissions, measure runtime and accuracy for both options, and decide if the tradeoff is worthwhile. looking at the theory, smoothing the emission probabilities is expected to cause a substantial increase in runtime, because now you are calculating paths into every tag for every word, instead of throwing away most of the paths, it also has the potential for an improvement in accuracy, since you may run into a situation where context strongly suggests a tag but the current word was only seen with other tags. however, the improvement in accuracy is likely to be fairly modest, because if a word was seen with some tags but not others, then either the word is rare or the other tags are rare for that word.\n",
      "\n",
      "Top most similar posts:\n",
      "['laplace smoothing was covered in the first week of class @490, and again last week (lecture 13 on october 4). smoothing each row just means treating each outgoing transition separately. that is, we perform smoothing on all the transitions from noun; separately, we perform smoothing on all the transitions from verb; and so on. because each of these is a separate probability distribution.', 'yes, since any other value of the emissions probability would lead to some sort of influence of the overall state probability. for unseen words, i just set emission probability to 1 to account for that.', 'when there are many transitions whose probabilities are zero, it can also lead to dividing by zero. to avoid this, we add 1 to the numerator and some factor to the denominator. this is called laplace smoothing. we do not add 1 to that number. otherwise, the probability would become greater than 1', \"smoothing the emissions matrix would sacrifice efficiency due to its size. edit to add further explanation: you only need to smooth the emission matrix for unseen words. this is because smoothing the entire emissions matrix will assign too much probability to unseen events. that is, smoothing will result in a words having probabilities for tags they were not associated with in the training. thus, sacrificing accuracy in addition to efficiency. smoothing is typically done to either the transition or the emissions matrix. transition is typically chosen because some probabilities need to be reserved as unseen. for example, what if a noun/verb pair were never seen in the training data? does that mean a noun/verb pair will never appear while testing? take the sentence 'dogs lick'. the model would not be able to assign a non-zero transition probability between dogs/noun -> lick/verb. therefore, the absence of smoothing would lead to the verb being mistagged, thus impacting accuracy.\"]\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: states vs observations.the parts of speech themselves are the states correct? which would make each word the observation? i am confused because there is a possibility our input data does not contain all possible part of speech tags and therefore will always incorrectly label any words belonging to that part of speech. is there a known list of part of speech tags we are supposed to use? or just use the ones from the training data?\n",
      "Expected Answer: yes and yes. i am not sure if the data we are working with will have such cases. but if we do, then you are correct. it can happen that we do not see a tag in train dataset because of which we assign words belonging to that tag incorrectly when tagging test dataset. no there is no known list of part of speech tags. we should be using and learning model from the ones that are present in training data.\n",
      "\n",
      "Top most similar posts:\n",
      "['yes and yes. i am not sure if the data we are working with will have such cases. but if we do, then you are correct. it can happen that we do not see a tag in train dataset because of which we assign words belonging to that tag incorrectly when tagging test dataset. no there is no known list of part of speech tags. we should be using and learning model from the ones that are present in training data.', 'i do not think we are supposed to do this because stopwords too have a part of speech associated with them. so we are needed to predict that. also, these stopwords and predictions define the probability of certain pos, if you remove them you model will not take this into account. certain pos can have more probability to end or start the sentence than others. removing stopwords, punctuation etc will ignore this. i think stemming too should not be done as it can change the pos tag. please correct me if i am wrong.', 'finding open-class parts of speech.i am trying to find a good way to determine if a part of speech can be considered open-class. are there any general rules to follow when doing this? i tried calculating the average vocabulary size of each part of speech, and then classifying everything with a greater than average vocab size as open-class. however, this greatly reduced the algorithms accuracy, so i suspect this is the wrong way to go about it.', 'we talked about that in the \"subword representation, contextualized representation\" lecture. please refer to those slides.']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: is brill tagger a discriminative model or generative model?.\n",
      "Expected Answer: neither. the model does not output probabilities of tags or sequences of tags.\n",
      "\n",
      "Top most similar posts:\n",
      "['neither. the model does not output probabilities of tags or sequences of tags.', 'not a pure rule-based model. it is a combination of a statistical tagger and a rule-based tagger that learns rules automatically from the data.', 'does encoder (decoder) imply a specific model or just a general speaking of sentence representation?.in the class, prof. chen talked about bi-encoder & cross-encoder, and i am confused that if the encoder is a specific model or it could be any kind of sentence representation method? for example, the output of lstm, or cnn, or self-attention? since the key word \"encoder\" (and \"decoder\") was mentioned several times during the lecture, i want to figure out what it really means. thanks in advance!', 'yes and yes. i am not sure if the data we are working with will have such cases. but if we do, then you are correct. it can happen that we do not see a tag in train dataset because of which we assign words belonging to that tag incorrectly when tagging test dataset. no there is no known list of part of speech tags. we should be using and learning model from the ones that are present in training data.']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: finding open-class parts of speech.i am trying to find a good way to determine if a part of speech can be considered open-class. are there any general rules to follow when doing this? i tried calculating the average vocabulary size of each part of speech, and then classifying everything with a greater than average vocab size as open-class. however, this greatly reduced the algorithms accuracy, so i suspect this is the wrong way to go about it.\n",
      "Expected Answer: the open/closed class distinction should not greatly improve or greatly decrease accuracy. remember, it only applies to unseen words, and there are not that many of those, so the effect should be rather small. from a good implementation of the open/closed class distinction you can expect an improvement of about 1 percentage point. if you see a great reduction in accuracy, then my first suspicion would be a bug in the implementation.\n",
      "\n",
      "Top most similar posts:\n",
      "['the open/closed class distinction should not greatly improve or greatly decrease accuracy. remember, it only applies to unseen words, and there are not that many of those, so the effect should be rather small. from a good implementation of the open/closed class distinction you can expect an improvement of about 1 percentage point. if you see a great reduction in accuracy, then my first suspicion would be a bug in the implementation.', 'states vs observations.the parts of speech themselves are the states correct? which would make each word the observation? i am confused because there is a possibility our input data does not contain all possible part of speech tags and therefore will always incorrectly label any words belonging to that part of speech. is there a known list of part of speech tags we are supposed to use? or just use the ones from the training data?', 'yes and yes. i am not sure if the data we are working with will have such cases. but if we do, then you are correct. it can happen that we do not see a tag in train dataset because of which we assign words belonging to that tag incorrectly when tagging test dataset. no there is no known list of part of speech tags. we should be using and learning model from the ones that are present in training data.', 'by the way. i just exempted this question for all. i agree that a multi-class classifier is also not the best kind of solution.']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: hmm smoothing.hi! i went through the algorithm in the references given to us and although there are tables showing the viterbi trellis for an example with and without smoothing, i do not particularly see how this smoothing is done. are we allowed to look for references on how smoothing is applied to the computation of transition probabilities or can you point some of the references we are allowed to use to this end? thanks!\n",
      "Expected Answer: smoothing was covered in the first week of class, when talking about naive bayes. you can see descriptions of the process in jurafsky and martin, chapter 4, pages 5–6, and in manning, raghavan and schutze, chapter 13, page 260. smoothing for hmm works exactly the same way. you just need to make sure to separately smooth each row in the matrix, since each row represents an independent probability distribution. you are allowed to consult references. what is not allowed is consulting implementations (or texts that specifically describe an implementation).\n",
      "\n",
      "Top most similar posts:\n",
      "['hmm smoothing.hi! i went through the algorithm in the references given to us and although there are tables showing the viterbi trellis for an example with and without smoothing, i do not particularly see how this smoothing is done. are we allowed to look for references on how smoothing is applied to the computation of transition probabilities or can you point some of the references we are allowed to use to this end? thanks!', 'smoothing transition probability matrix.what is meant by smoothing each row in the transition probability matrix? if we are generally dividing by the number of times the previous tag is in the corpus, are we adding 1 to that number?', 'smoothing was covered in the first week of class, when talking about naive bayes. you can see descriptions of the process in jurafsky and martin, chapter 4, pages 5–6, and in manning, raghavan and schutze, chapter 13, page 260. smoothing for hmm works exactly the same way. you just need to make sure to separately smooth each row in the matrix, since each row represents an independent probability distribution. you are allowed to consult references. what is not allowed is consulting implementations (or texts that specifically describe an implementation).', 'laplace smoothing was covered in the first week of class @490, and again last week (lecture 13 on october 4). smoothing each row just means treating each outgoing transition separately. that is, we perform smoothing on all the transitions from noun; separately, we perform smoothing on all the transitions from verb; and so on. because each of these is a separate probability distribution.', 'sophisticated smoothing could lead to a small improvement, but if the accuracy you are getting is not in the general range of the baseline or reference, then you probably have a bug or error, and finding and fixing that would lead to more improvement than fiddling with the smoothing method']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: question about initial probabilties.are initial probabilities calculated considering only the first word in a sentence, or do we use the markov assumption and consider each word in the sentence as a potential starting point? if it is the latter, then would every word in the sentence be considered as a potential starting point except the very last word in the sentence?\n",
      "Expected Answer: by initial probabilities, i assume you mean what jurafsky and martin call “an initial probability distribution over states” (top of page 9). this is the probability distribution of the first state in a sentence. to estimate the initial probability of state x, just take the number of sentences that start with x, and divide by the total number of sentences. do not forget to apply smoothing. an alternative approach is the algorithm presented in the written exercises that were distributed on blackboard. add a special initial state that is not associated with any observations, and assume that each sentence starts with a transition from that initial state to the first word-producing state. then the initial probability distribution from the previous paragraph becomes just another row in the transition matrix. the two approaches are mathematically equivalent.\n",
      "\n",
      "Top most similar posts:\n",
      "['by initial probabilities, i assume you mean what jurafsky and martin call “an initial probability distribution over states” (top of page 9). this is the probability distribution of the first state in a sentence. to estimate the initial probability of state x, just take the number of sentences that start with x, and divide by the total number of sentences. do not forget to apply smoothing. an alternative approach is the algorithm presented in the written exercises that were distributed on blackboard. add a special initial state that is not associated with any observations, and assume that each sentence starts with a transition from that initial state to the first word-producing state. then the initial probability distribution from the previous paragraph becomes just another row in the transition matrix. the two approaches are mathematically equivalent.', 'i might be wrong but i do not think the initial probability distribution has anything to do with the position of the words. i think we just calculate that depending upon the number of occurrences in the corpus.', 'i do not think we are supposed to do this because stopwords too have a part of speech associated with them. so we are needed to predict that. also, these stopwords and predictions define the probability of certain pos, if you remove them you model will not take this into account. certain pos can have more probability to end or start the sentence than others. removing stopwords, punctuation etc will ignore this. i think stemming too should not be done as it can change the pos tag. please correct me if i am wrong.', 'yes, since any other value of the emissions probability would lead to some sort of influence of the overall state probability. for unseen words, i just set emission probability to 1 to account for that.']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: can anyone explain q1?.why chi-squared test is the most suitable hypothesis test to know if gender has anything to do with political party preference (democrats, republicans, etc)? why not t-test? what is the difference between chi-squared test and t-test?\n",
      "Expected Answer: the chi-squared test is used when we have categorical variables (here gender and political party preference). however, the t-test is used when we have quantitative variables (like to compare the mean of 2 samples).\n",
      "\n",
      "Top most similar posts:\n",
      "['can anyone explain q1?.why chi-squared test is the most suitable hypothesis test to know if gender has anything to do with political party preference (democrats, republicans, etc)? why not t-test? what is the difference between chi-squared test and t-test?', 'the chi-squared test is used when we have categorical variables (here gender and political party preference). however, the t-test is used when we have quantitative variables (like to compare the mean of 2 samples).', 'question 32.question 32 which of the following can be a (eey=eeonn selected answer: @ fact verification answers: @ abstractive summarization fact verification named entity recognition none of the other choices', 'for question 8, the question is talking about the semantic structure and not the syntactic structure. hence, this will be false!', 'so the question in quiz 9 should be false? the answer was true for the question: neural language models use word-embedding models in their training.']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: using cuda.when implementing fnn and rnn, do we have to cast tensors to a cuda type to run on gpu? thank you!\n",
      "Expected Answer: yes\n",
      "\n",
      "Top most similar posts:\n",
      "['training on rnn and gru too slow even using gpu.do we need to iterate through entire training dataset in one epoch? it takes about 1 hr to run 10 epoch for rnn which i directly follow the tutorial in the pdf and add a optimizer. but in the tutorial seems only train on one document for one epoch. even worse for gru which i wait 20 minutes and does not finish one epoch. is there suggestion to solve this problem? by the way i choose hidden size as 30', 'depends. an rnn is simply a sequence model. if your input can handle oov tokens (eg. passing fasttext embs to rnn), then then the answer is no. if you are simply using something like word2vec or one hot then yes.', 'input to rnn.will the input of the rnn be a word embedding aka a vector of size (300,)? if yes then would not hidden state also have to be 300 ?', 'oh okay but i was thinking that the shape in rnn input would be different than that for the fnn as in fnn we were concatenating the w2v vectors resulting in each sequence of shape (3000) while in rnn i thought we are not concatenating but passing each word vector one by one for a given sequence thus each input sequence having the shape (20, 300) instead of (6000) which would have been if we concatenated the vectors.']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: quiz question - 9.hi, question 9: choice 'it is efficient to learn long-term dependencies through time' is considered as a wrong option in the rubrics. but rnn's are known for long term dependencies over time through their dynamic system. normalized rnn's are even more capable of unfolding long term dependences over time. here are some relevant papers: 1. 2. 3.\n",
      "Expected Answer: the key is the word \"efficient\". rnn can model the long-term dependencies but computationally the process is not efficient and that is why gated rnn is developed.\n",
      "\n",
      "Top most similar posts:\n",
      "['the key is the word \"efficient\". rnn can model the long-term dependencies but computationally the process is not efficient and that is why gated rnn is developed.', 'i second this! compared to fnn, rnns are know for capturing long term dependencies. the question does not specify whether rnns being compared to lstms or transformers.', 'query regarding rnns.do rnns have a fixed vocabulary limitation ?', 'weights during rnn backprop are not updated on each time step, since backprop happens after the full forward computation is done (i believe from slide 9 of lecture 6). that means that the same weights would actually be shared. therefore, i do not think this question was misleading. (for rnns, a \"timestep\" refers to the position of an individual rnn cell, not a full forward-backward computation.)']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: not using grad.zero() to initialize the gradients after each iteration?.if we do not use grad.zero() and allow the gradients to accumulate, can it be useful in some cases?\n",
      "Expected Answer: i have not seen such a case personally but likely there is a reason behind the pytorch designer team that set it to be accumulative.\n",
      "\n",
      "Top most similar posts:\n",
      "['vanishing gradient.do all models that use gradient descent, have a vanishing gradient problem?', 'it depends on the depth of the neural networks. consider a one-layer nn, there is no vanishing gradient problem. however, for a very depth model (consider a 100-layer fully connected nn), it is higher chance that there is a vanishing gradient problem.', 'gradient descent is a general optimization algorithm and works for all functions that are differentiable. so, you can use it for many problems. it may be possible to come up with a solution for perception using gradient descent, too. perceptron algorithm is specific to the perception model.', 'training on rnn and gru too slow even using gpu.do we need to iterate through entire training dataset in one epoch? it takes about 1 hr to run 10 epoch for rnn which i directly follow the tutorial in the pdf and add a optimizer. but in the tutorial seems only train on one document for one epoch. even worse for gru which i wait 20 minutes and does not finish one epoch. is there suggestion to solve this problem? by the way i choose hidden size as 30']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: vague quiz 3 question.during the quiz there was this t/f question that i felt was vague:in rnn, the weights during each time step is shared.i thought that this question was asking if the neurons are using the same weights at each time step, which i thought was false since we should be updating the weights and hence, using different weights at each time step. the correct answer for this question was true. i believe that this question is asking whether we are calculating the new weight at time t using the weights from time t-1. am i the only who felt that the question was a little misleading?\n",
      "Expected Answer: weights during rnn backprop are not updated on each time step, since backprop happens after the full forward computation is done (i believe from slide 9 of lecture 6). that means that the same weights would actually be shared. therefore, i do not think this question was misleading. (for rnns, a \"timestep\" refers to the position of an individual rnn cell, not a full forward-backward computation.)\n",
      "\n",
      "Top most similar posts:\n",
      "['weights during rnn backprop are not updated on each time step, since backprop happens after the full forward computation is done (i believe from slide 9 of lecture 6). that means that the same weights would actually be shared. therefore, i do not think this question was misleading. (for rnns, a \"timestep\" refers to the position of an individual rnn cell, not a full forward-backward computation.)', 'so the question in quiz 9 should be false? the answer was true for the question: neural language models use word-embedding models in their training.', \"quiz question - 9.hi, question 9: choice 'it is efficient to learn long-term dependencies through time' is considered as a wrong option in the rubrics. but rnn's are known for long term dependencies over time through their dynamic system. normalized rnn's are even more capable of unfolding long term dependences over time. here are some relevant papers: 1. 2. 3.\", 'for question 8, the question is talking about the semantic structure and not the syntactic structure. hence, this will be false!']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: cosine similarity numerical.i just wanted to know if this is correct question : find cosine similarity for : x1 = (3,4) x2 = (1,0) solution : x*y cosine similarity = ________ ||x|| ||y|| 3*1 + 4*0 ___________________________ = 3/5 = 0.6 sqrt((3*3+4*4)x(1*1+0*0)) can you please confirm this calculation or is it incomplete as do we consider the euclidean distance instead, by assuming center to be the origin? (3-0)*(1-0) + (4-0)*(0-0) ___________________________ = 3/2 = 0.6 sqrt((3*3+4*4)x(1*1+0*0)) do we consider the center to be (0,0) always, as it was not mentioned by default in the question in the quiz? thank you\n",
      "Expected Answer: the first computation is correct and the definitions of dot product and vector norm are both independent of the center of coordinate.\n",
      "\n",
      "Top most similar posts:\n",
      "['thank you so do we consider the euclidean distance while taking the magnitudes in the denominator is what i am confused about ? i wanted to know if magnitude of vectors i.e. |a| and |b| in the formula are absolute vectors or distances from a point of consideration?', 'the first computation is correct and the definitions of dot product and vector norm are both independent of the center of coordinate.', 'it should be 5 since it is a 5 class classification problem.', 'tried running the code on google collab, but it is running for more than 25mins now. just wanted to know if it is common for such a long runtime or if there is some mistake in the code.']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: tf-idf.should i run tf-idf and extract features treating entire dataset as corpus? or by doing it classwise??\n",
      "Expected Answer: you should do it on the entire dataset.\n",
      "\n",
      "Top most similar posts:\n",
      "['tf-idf.should i run tf-idf and extract features treating entire dataset as corpus? or by doing it classwise??', 'feature extraction and classifying using one model.since i use tfidf for feature extraction, i need to keep the bag of words in the model for feature extraction in classifying right? does that mean i also need to store the idf for each word calculated in training and load it for classification? since the testing data need the same idf for the words in my bag of words. (unseen words will be discarded)', 'tf-idf.why do we use a logarithm for idf? how does it benefit mathematics', 'how to use word tokens as features.in the overview of code assignment 4, it said that \"you may use the word tokens as features. \". professor said i can count words instead of using tf-idf. can anybody give me an example about how to transfer a review (i.e. a sequence of words/strings) into a vector (a sequence of numbers)?', 'it can be because you are using a non-sparse matrix format. tfidf features can be stored as sparse matrices.']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: gradient descent vs perceptron algorithms.i want to verify my understanding is correct. the perceptron algorithm describes the full process of assigning random weights, then viewing examples from training data, adjusting weights, etc. gradient descent is a specific optimization technique that can be used to adjust weights. so one could replace the \"vanilla\" method of adjusting weights from the perceptron algorithm with gradient descent?\n",
      "Expected Answer: gradient descent is a general optimization algorithm and works for all functions that are differentiable. so, you can use it for many problems. it may be possible to come up with a solution for perception using gradient descent, too. perceptron algorithm is specific to the perception model.\n",
      "\n",
      "Top most similar posts:\n",
      "['gradient descent is a general optimization algorithm and works for all functions that are differentiable. so, you can use it for many problems. it may be possible to come up with a solution for perception using gradient descent, too. perceptron algorithm is specific to the perception model.', 'regarding random shuffling between epochs while training.i am getting better mean f1 scores for both vanilla and averaged perceptron without random shuffling the dataset between epochs while training. is shuffling between epochs mandatory while training? can i train the perceptron without shuffling the dataset?', 'not using grad.zero() to initialize the gradients after each iteration?.if we do not use grad.zero() and allow the gradients to accumulate, can it be useful in some cases?', 'weights during rnn backprop are not updated on each time step, since backprop happens after the full forward computation is done (i believe from slide 9 of lecture 6). that means that the same weights would actually be shared. therefore, i do not think this question was misleading. (for rnns, a \"timestep\" refers to the position of an individual rnn cell, not a full forward-backward computation.)']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: lemmatization based on pos tag.the ntlk lemmatizer will not lemmatize \"wearing\" to \"wear\" unless explicit mentioned to considered it as a word, because the default tag is \"noun\" . is this lemmatization acceptable?\n",
      "Expected Answer: i think so. you do not have to look at the specific cases of lemmatization otherwise it would be too much work, the default should suffice.\n",
      "\n",
      "Top most similar posts:\n",
      "['i think so. you do not have to look at the specific cases of lemmatization otherwise it would be too much work, the default should suffice.', 'lemmatization is performed on a word-level basis.', 'you can make a string from words after lemmatization.', 'yes and yes. i am not sure if the data we are working with will have such cases. but if we do, then you are correct. it can happen that we do not see a tag in train dataset because of which we assign words belonging to that tag incorrectly when tagging test dataset. no there is no known list of part of speech tags. we should be using and learning model from the ones that are present in training data.']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: why do we use perplexity and not raw probability to evaluate language models?.why do we use perplexity and not raw probability to evaluate language models?\n",
      "Expected Answer: we will cover this topic when we cover language models. in short, because raw probability values are between 0 and 1 and does not convey an intuitive measure.\n",
      "\n",
      "Top most similar posts:\n",
      "['we will cover this topic when we cover language models. in short, because raw probability values are between 0 and 1 and does not convey an intuitive measure.', 'neither. the model does not output probabilities of tags or sequences of tags.', 'we talked about that in the \"subword representation, contextualized representation\" lecture. please refer to those slides.', 'i might be wrong but i do not think the initial probability distribution has anything to do with the position of the words. i think we just calculate that depending upon the number of occurrences in the corpus.']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: bow dictionary.does the dictionary need to be ordered in some way for bow to be used ? if so, is there any advantage to ordering it in alphabetical way ?\n",
      "Expected Answer: ordering should not have any effect when building bow. unless you are dealing with billions of reviews, even in such case ordering your bow w.r.t word frequency( to reduce document traversal) might not have any significant effect.\n",
      "\n",
      "Top most similar posts:\n",
      "['ordering should not have any effect when building bow. unless you are dealing with billions of reviews, even in such case ordering your bow w.r.t word frequency( to reduce document traversal) might not have any significant effect.', 'in this case, the feature might not be a good representation of the document. you can expand the vocabulary to make the representation less sparse, at the cost of increasing the dimension of the vector. bow is a simple model, it suffers from the problems like the one you mentioned inherently. it is useful in some simple applications, when you do not have to deal with the outliers.', 'i might be wrong but i do not think the initial probability distribution has anything to do with the position of the words. i think we just calculate that depending upon the number of occurrences in the corpus.', 'syntax is a matter of sentence structure, as explained in the introduction class, the parsing class, and the associated reading. while it is true that different structures often correspond to different meanings, this is not always the case. for example, \"(a cat and a dog) and a mouse\" has a very similar meaning to \"a cat and (a dog and a mouse)\".']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: bag of words.how is a model which encounters a vector space representation of word vs document which is extremely sparse affected? what can be done to handle the negative effects on the model?\n",
      "Expected Answer: in this case, the feature might not be a good representation of the document. you can expand the vocabulary to make the representation less sparse, at the cost of increasing the dimension of the vector. bow is a simple model, it suffers from the problems like the one you mentioned inherently. it is useful in some simple applications, when you do not have to deal with the outliers.\n",
      "\n",
      "Top most similar posts:\n",
      "['in this case, the feature might not be a good representation of the document. you can expand the vocabulary to make the representation less sparse, at the cost of increasing the dimension of the vector. bow is a simple model, it suffers from the problems like the one you mentioned inherently. it is useful in some simple applications, when you do not have to deal with the outliers.', 'feature vector for unseen words.as we are creating one-hot encoding to convert reviews into features, how can i handle any unseen words in the test dataset? should i skip the word? or is there any other better alternative?', 'using words as features, each word can be considered a feature, and the count of the word is the value. so a review like \"the omni is the best hotel in the world\" will have the following feature values: {\\'best\\': 1, \\'hotel\\': 1, \\'in\\': 1, \\'is\\': 1, \\'omni\\': 1, \\'the\\': 3, \\'world\\': 1} if you want to think of this as a fixed-length vector, then the length would be the entire vocabulary, and most values will be zero. notice how in the feature structure above, i silently changed \\'the\\' to \\'the\\', but kept \\'omni\\' without change. this would be difficult to do programmatically, but this is one of the things to experiment with: is it helpful to change case? mess around with suffixes? do something with punctuation? is it helpful to develop features that are not words? for example, use word bigrams as features? are there any words that should be excluded from being used as features? i recommend starting with very simple features (say, just words), in order to get the basic logic of the perceptron working, once you have that, you can experiment to see how changes to features affect performance of the perceptron. no, word2vec is not allowed for this assignment.', 'yes, word embedding are not used to rain word2vec. we only use one-hot vectors and then learn word embeddings as the weights of the two-layer network.']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: tf-idf.why do we use a logarithm for idf? how does it benefit mathematics\n",
      "Expected Answer: it suppresses large values to avoid making some feature values that correspond to infrequent ones too large. in practice, this has impression been found to be helpful.\n",
      "\n",
      "Top most similar posts:\n",
      "['tf-idf.should i run tf-idf and extract features treating entire dataset as corpus? or by doing it classwise??', 'how to use word tokens as features.in the overview of code assignment 4, it said that \"you may use the word tokens as features. \". professor said i can count words instead of using tf-idf. can anybody give me an example about how to transfer a review (i.e. a sequence of words/strings) into a vector (a sequence of numbers)?', 'it can be because you are using a non-sparse matrix format. tfidf features can be stored as sparse matrices.', 'feature extraction and classifying using one model.since i use tfidf for feature extraction, i need to keep the bag of words in the model for feature extraction in classifying right? does that mean i also need to store the idf for each word calculated in training and load it for classification? since the testing data need the same idf for the words in my bag of words. (unseen words will be discarded)']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: tokenize review before lemmatization.should we tokenize review_body before lemmatizing the sentence?\n",
      "Expected Answer: lemmatization is performed on a word-level basis.\n",
      "\n",
      "Top most similar posts:\n",
      "['should the lemmatized review (after lemmatizing) be a python list of lemmatized words? thanks!', 'does lemmatization take a long time for 100000 reviews? in my case, the code is running for a while now, not sure if its common.', 'performing lemmatization on the whole of reviews dataset.the lemmatization process is taking a long time. has anyone faced this issue? anything that can done to improve the runtime?', 'how to use word tokens as features.in the overview of code assignment 4, it said that \"you may use the word tokens as features. \". professor said i can count words instead of using tf-idf. can anybody give me an example about how to transfer a review (i.e. a sequence of words/strings) into a vector (a sequence of numbers)?']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: contractions library.do you all have any other recommendations besides the standard contractions library (with which you can do contractions.fix)? i noticed online that there are issues with it when dealing with ambiguity in tense and all. or do you all think this can be safely ignored with respect to the performance of the various models?\n",
      "Expected Answer: the standard library should be fine for a decent performance but if you think you can do better, you can use an improved approach on top of it.\n",
      "\n",
      "Top most similar posts:\n",
      "['the standard library should be fine for a decent performance but if you think you can do better, you can use an improved approach on top of it.', 'tensorflow or pytorch?.hello, tensorflow and pytorch seem to be the most popular ml libraries today. is there any recommendation on what is more suitable for nlp tasks? in terms of learning curve, industry-relevance, ease of deployment, etc.? thanks.', 'historically, pytorch had a better performance on recursive networks (e.g. lstm) that were dominant before transformers, but they are both pretty good now. i personally use pytorch as it feels more natural to me but tf2.0 is also pretty mature and powerful once you get into the mindset. i cannot comment about industry usage. so just pick one.', 'i am on colab as well, but lemmatizing (including nouns, verbs and adjectives) takes like 2 mins']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: performing lemmatization on the whole of reviews dataset.the lemmatization process is taking a long time. has anyone faced this issue? anything that can done to improve the runtime?\n",
      "Expected Answer: running time depends on your machine, too. but generally, it is not superfast and we do not penaliz you if your running time is long.\n",
      "\n",
      "Top most similar posts:\n",
      "['does lemmatization take a long time for 100000 reviews? in my case, the code is running for a while now, not sure if its common.', 'should the lemmatized review (after lemmatizing) be a python list of lemmatized words? thanks!', 'i am on colab as well, but lemmatizing (including nouns, verbs and adjectives) takes like 2 mins', 'tokenize review before lemmatization.should we tokenize review_body before lemmatizing the sentence?']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: different preprocessing steps for each algorithm.hello, can we have different preprocessing steps for different algorithms given we explain the steps in detail in report and including/excluding some preprocessing steps improved scores. or is it required that all data go through same set of preprocessing steps and then each algorithm is evaluated on it?\n",
      "Expected Answer: yes, that is possible.\n",
      "\n",
      "Top most similar posts:\n",
      "['pos tagging: preprocessing.is preprocessing generally done with pos tagging? such as removing stop words, removing punctuations, or stemming to name a few.thanks', 'i think so. you do not have to look at the specific cases of lemmatization otherwise it would be too much work, the default should suffice.', 'you should do it on the entire dataset.', 'i also have this question as i came up with an iterative model that produced decent results']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: what is the number of fnn output layer.like we set first hidden layer as 50, the second hidden layer is 10. but how about the output layer? is it 5 or 1?\n",
      "Expected Answer: it should be 5 since it is a 5 class classification problem.\n",
      "\n",
      "Top most similar posts:\n",
      "['just same as the fnn, we do not need to modify the dimensions except hidden state, as the tutorial initial it as (1,hiddensize), so i changed it to (batchsize,hiddensize)', 'i suppose the hidden layer to have a shape like (num_layers, time_steps(i.e. 20), output_embedding) now the output embedding can be of your choice. just like the hidden layer output dimensions of mlp', 'when i set the output layer as 5, i got runtimeerror: 0d or 1d target tensor expected, multi-target not supported does anyone know why?', 'since it is a 5-class multiclassification problem, the output of the neural network should be in the range between 0 to 4. but in our data, we have ratings between 1 to 5. this is the problem.']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: input to rnn.will the input of the rnn be a word embedding aka a vector of size (300,)? if yes then would not hidden state also have to be 300 ?\n",
      "Expected Answer: i suppose the hidden layer to have a shape like (num_layers, time_steps(i.e. 20), output_embedding) now the output embedding can be of your choice. just like the hidden layer output dimensions of mlp\n",
      "\n",
      "Top most similar posts:\n",
      "['depends. an rnn is simply a sequence model. if your input can handle oov tokens (eg. passing fasttext embs to rnn), then then the answer is no. if you are simply using something like word2vec or one hot then yes.', 'oh okay but i was thinking that the shape in rnn input would be different than that for the fnn as in fnn we were concatenating the w2v vectors resulting in each sequence of shape (3000) while in rnn i thought we are not concatenating but passing each word vector one by one for a given sequence thus each input sequence having the shape (20, 300) instead of (6000) which would have been if we concatenated the vectors.', 'query regarding rnns.do rnns have a fixed vocabulary limitation ?', 'could you please elaborate on how you used batched processing with rnn? i am kind of stuck here. not sure how to change the dimensions of the input, hidden and output layers when using batches.']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: training on rnn and gru too slow even using gpu.do we need to iterate through entire training dataset in one epoch? it takes about 1 hr to run 10 epoch for rnn which i directly follow the tutorial in the pdf and add a optimizer. but in the tutorial seems only train on one document for one epoch. even worse for gru which i wait 20 minutes and does not finish one epoch. is there suggestion to solve this problem? by the way i choose hidden size as 30\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: does anyone have the same problem that rnn and gru loss does not decrease?.i trained for 100 epoch and it keeps at a very high value same as beginning. stuck here and debug for hours and did not see a difference. does anyone have the same problem? any suggestion?\n",
      "Expected Answer: maybe change the learning rate and see if it helps.\n",
      "\n",
      "Top most similar posts:\n",
      "['training on rnn and gru too slow even using gpu.do we need to iterate through entire training dataset in one epoch? it takes about 1 hr to run 10 epoch for rnn which i directly follow the tutorial in the pdf and add a optimizer. but in the tutorial seems only train on one document for one epoch. even worse for gru which i wait 20 minutes and does not finish one epoch. is there suggestion to solve this problem? by the way i choose hidden size as 30', 'with more epoch validation accuracy is going down for dev set.why is that with more epochs the validation accuracy goes down for dev set ? what is the reason behind it? should i decrease the epoch value ? will it work better on test set ?', 'this is the case of overfitting. over a certain number of epochs, the model starts to capture the noise in the training data and thus resulting in low accuracy in validation data.', 'could you please elaborate on how you used batched processing with rnn? i am kind of stuck here. not sure how to change the dimensions of the input, hidden and output layers when using batches.']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: question 47. bert cannot be fine-tuned to abstractive summarization because it could fill the masked words but cannot generate a summary with a decoder? thanks a lot!\n",
      "Expected Answer: for abstractive sumarization you need an autoregressive model. bert is an mlm and does not do autoregressive generation.\n",
      "\n",
      "Top most similar posts:\n",
      "['question 47.question 47 which of the following cannot be fine-tuned into an abstractive summarization model? 3 selected answer: @ transformer encoder-decoder answers: @ bert bart 15 transformer encoder-decoder', 'for abstractive sumarization you need an autoregressive model. bert is an mlm and does not do autoregressive generation.', 'question 32. can anyone explain why abstractive summarization could be a use case of nli? and for fact verification, we could have a context and a fact sentence, which we could do nli in my opinion? thank you very much!', 'we talked about that in the \"subword representation, contextualized representation\" lecture. please refer to those slides.']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: question 32. can anyone explain why abstractive summarization could be a use case of nli? and for fact verification, we could have a context and a fact sentence, which we could do nli in my opinion? thank you very much!\n",
      "Expected Answer: that should be indeed fact verification. i just corrected that in the system.\n",
      "\n",
      "Top most similar posts:\n",
      "['question 47.question 47 which of the following cannot be fine-tuned into an abstractive summarization model? 3 selected answer: @ transformer encoder-decoder answers: @ bert bart 15 transformer encoder-decoder', 'question 47. bert cannot be fine-tuned to abstractive summarization because it could fill the masked words but cannot generate a summary with a decoder? thanks a lot!', 'question 32.question 32 which of the following can be a (eey=eeonn selected answer: @ fact verification answers: @ abstractive summarization fact verification named entity recognition none of the other choices', 'for abstractive sumarization you need an autoregressive model. bert is an mlm and does not do autoregressive generation.']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: question 15. i am confused about this question. in the class, the example of syntactic ambiguity is that \"flying planes can be dangerous\". and professor ron mentioned that words in this string have one meaning but the combination of words has more than one meaning. so i guess my choice also makes sense?\n",
      "Expected Answer: syntax is a matter of sentence structure, as explained in the introduction class, the parsing class, and the associated reading. while it is true that different structures often correspond to different meanings, this is not always the case. for example, \"(a cat and a dog) and a mouse\" has a very similar meaning to \"a cat and (a dog and a mouse)\".\n",
      "\n",
      "Top most similar posts:\n",
      "['question 15. i am confused about this question. in the class, the example of syntactic ambiguity is that \"flying planes can be dangerous\". and professor ron mentioned that words in this string have one meaning but the combination of words has more than one meaning. so i guess my choice also makes sense?', 'question 15.question 15 ‘syntactic ambiguity describes the situation where a string of words: 3 selected answer: ¢ can have more than one meaning answers: cannot be interpreted using syntactic rules can have more than one meaning @ can correspond to more than one structure can have dozens of implau', 'syntax is a matter of sentence structure, as explained in the introduction class, the parsing class, and the associated reading. while it is true that different structures often correspond to different meanings, this is not always the case. for example, \"(a cat and a dog) and a mouse\" has a very similar meaning to \"a cat and (a dog and a mouse)\".', 'for question 8, the question is talking about the semantic structure and not the syntactic structure. hence, this will be false!', 'using words as features, each word can be considered a feature, and the count of the word is the value. so a review like \"the omni is the best hotel in the world\" will have the following feature values: {\\'best\\': 1, \\'hotel\\': 1, \\'in\\': 1, \\'is\\': 1, \\'omni\\': 1, \\'the\\': 3, \\'world\\': 1} if you want to think of this as a fixed-length vector, then the length would be the entire vocabulary, and most values will be zero. notice how in the feature structure above, i silently changed \\'the\\' to \\'the\\', but kept \\'omni\\' without change. this would be difficult to do programmatically, but this is one of the things to experiment with: is it helpful to change case? mess around with suffixes? do something with punctuation? is it helpful to develop features that are not words? for example, use word bigrams as features? are there any words that should be excluded from being used as features? i recommend starting with very simple features (say, just words), in order to get the basic logic of the perceptron working, once you have that, you can experiment to see how changes to features affect performance of the perceptron. no, word2vec is not allowed for this assignment.']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: how to handle oovs?.what are the different ways to handle oov (out of vocabulary) words?\n",
      "Expected Answer: we talked about that in the \"subword representation, contextualized representation\" lecture. please refer to those slides.\n",
      "\n",
      "Top most similar posts:\n",
      "['using words as features, each word can be considered a feature, and the count of the word is the value. so a review like \"the omni is the best hotel in the world\" will have the following feature values: {\\'best\\': 1, \\'hotel\\': 1, \\'in\\': 1, \\'is\\': 1, \\'omni\\': 1, \\'the\\': 3, \\'world\\': 1} if you want to think of this as a fixed-length vector, then the length would be the entire vocabulary, and most values will be zero. notice how in the feature structure above, i silently changed \\'the\\' to \\'the\\', but kept \\'omni\\' without change. this would be difficult to do programmatically, but this is one of the things to experiment with: is it helpful to change case? mess around with suffixes? do something with punctuation? is it helpful to develop features that are not words? for example, use word bigrams as features? are there any words that should be excluded from being used as features? i recommend starting with very simple features (say, just words), in order to get the basic logic of the perceptron working, once you have that, you can experiment to see how changes to features affect performance of the perceptron. no, word2vec is not allowed for this assignment.', 'how do we handle out of vocabulary words in the above approach? if our training set does not contain a word encountered during testing is there a preferred way to handle them? or will the testing set not have words not encountered in training?', 'ignoring an unknown word is probably the simplest solution. alternatively, you can devise features that could be triggered by both known and unknown words.', 'bag of words.how is a model which encounters a vector space representation of word vs document which is extremely sparse affected? what can be done to handle the negative effects on the model?']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: overfitting?.hi, in this loss graph, the validation loss increases, but validation accuracy also increases. is this overfitting or just the prediction power of classes shifting?\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: with more epoch validation accuracy is going down for dev set.why is that with more epochs the validation accuracy goes down for dev set ? what is the reason behind it? should i decrease the epoch value ? will it work better on test set ?\n",
      "Expected Answer: this is the case of overfitting. over a certain number of epochs, the model starts to capture the noise in the training data and thus resulting in low accuracy in validation data.\n",
      "\n",
      "Top most similar posts:\n",
      "['this is the case of overfitting. over a certain number of epochs, the model starts to capture the noise in the training data and thus resulting in low accuracy in validation data.', 'overfitting?.hi, in this loss graph, the validation loss increases, but validation accuracy also increases. is this overfitting or just the prediction power of classes shifting?', 'does anyone have the same problem that rnn and gru loss does not decrease?.i trained for 100 epoch and it keeps at a very high value same as beginning. stuck here and debug for hours and did not see a difference. does anyone have the same problem? any suggestion?', 'training on rnn and gru too slow even using gpu.do we need to iterate through entire training dataset in one epoch? it takes about 1 hr to run 10 epoch for rnn which i directly follow the tutorial in the pdf and add a optimizer. but in the tutorial seems only train on one document for one epoch. even worse for gru which i wait 20 minutes and does not finish one epoch. is there suggestion to solve this problem? by the way i choose hidden size as 30']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: knn classification k = 1.is there a situation that using k = 1 is acceptable for knn. if k=1 gives the highest model performance on an “unseen” dev set, does it make sense to use k=1?\n",
      "Expected Answer: in that case you do not really need to think about using an ml model. if your real world data maps 1:1 to your train data, you might as well write a sql(elasticsearch would be better) query with training data as your database.\n",
      "\n",
      "Top most similar posts:\n",
      "['with more epoch validation accuracy is going down for dev set.why is that with more epochs the validation accuracy goes down for dev set ? what is the reason behind it? should i decrease the epoch value ? will it work better on test set ?', 'you should do it on the entire dataset.', 'it should be 5 since it is a 5 class classification problem.', 'it suppresses large values to avoid making some feature values that correspond to infrequent ones too large. in practice, this has impression been found to be helpful.']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: inter-rather agreement vs inter-annotator agreement.hello,are these two terms the same thing — just iaa being a bit more specified for annotation? otherwise i have seen them be used interchangeably in many places, and are measures with the same method’s, like cohen’s kappa.\n",
      "Expected Answer: same\n",
      "\n",
      "Top most similar posts:\n",
      "['inter-rather agreement vs inter-annotator agreement.hello,are these two terms the same thing — just iaa being a bit more specified for annotation? otherwise i have seen them be used interchangeably in many places, and are measures with the same method’s, like cohen’s kappa.', 'question 32. can anyone explain why abstractive summarization could be a use case of nli? and for fact verification, we could have a context and a fact sentence, which we could do nli in my opinion? thank you very much!', 'question 32.question 32 which of the following can be a (eey=eeonn selected answer: @ fact verification answers: @ abstractive summarization fact verification named entity recognition none of the other choices', 'neither. the model does not output probabilities of tags or sequences of tags.', 'we talked about that in the \"subword representation, contextualized representation\" lecture. please refer to those slides.']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: tensorflow or pytorch?.hello, tensorflow and pytorch seem to be the most popular ml libraries today. is there any recommendation on what is more suitable for nlp tasks? in terms of learning curve, industry-relevance, ease of deployment, etc.? thanks.\n",
      "Expected Answer: historically, pytorch had a better performance on recursive networks (e.g. lstm) that were dominant before transformers, but they are both pretty good now. i personally use pytorch as it feels more natural to me but tf2.0 is also pretty mature and powerful once you get into the mindset. i cannot comment about industry usage. so just pick one.\n",
      "\n",
      "Top most similar posts:\n",
      "['historically, pytorch had a better performance on recursive networks (e.g. lstm) that were dominant before transformers, but they are both pretty good now. i personally use pytorch as it feels more natural to me but tf2.0 is also pretty mature and powerful once you get into the mindset. i cannot comment about industry usage. so just pick one.', 'pytorch', 'nltk has a built-in function that you can use but you are free to use your own developed function if it leads to improved performance.', 'the standard library should be fine for a decent performance but if you think you can do better, you can use an improved approach on top of it.']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: pos tagging: preprocessing.is preprocessing generally done with pos tagging? such as removing stop words, removing punctuations, or stemming to name a few.thanks\n",
      "Expected Answer: i do not think we are supposed to do this because stopwords too have a part of speech associated with them. so we are needed to predict that. also, these stopwords and predictions define the probability of certain pos, if you remove them you model will not take this into account. certain pos can have more probability to end or start the sentence than others. removing stopwords, punctuation etc will ignore this. i think stemming too should not be done as it can change the pos tag. please correct me if i am wrong.\n",
      "\n",
      "Top most similar posts:\n",
      "['i do not think we are supposed to do this because stopwords too have a part of speech associated with them. so we are needed to predict that. also, these stopwords and predictions define the probability of certain pos, if you remove them you model will not take this into account. certain pos can have more probability to end or start the sentence than others. removing stopwords, punctuation etc will ignore this. i think stemming too should not be done as it can change the pos tag. please correct me if i am wrong.', 'different preprocessing steps for each algorithm.hello, can we have different preprocessing steps for different algorithms given we explain the steps in detail in report and including/excluding some preprocessing steps improved scores. or is it required that all data go through same set of preprocessing steps and then each algorithm is evaluated on it?', 'lemmatization based on pos tag.the ntlk lemmatizer will not lemmatize \"wearing\" to \"wear\" unless explicit mentioned to considered it as a word, because the default tag is \"noun\" . is this lemmatization acceptable?', 'dealing with unknown words in test data.. does \"ignoring emission prob\" mean considering it 1? eg. because for any tag, the prob of reaching it from a prev_tag is: prev_prob*emission_prob*transition_prob. which means the final prob will be prev_prob*transition_prob does ignoring mean considering emission prob as 1? thank you!']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: does encoder (decoder) imply a specific model or just a general speaking of sentence representation?.in the class, prof. chen talked about bi-encoder & cross-encoder, and i am confused that if the encoder is a specific model or it could be any kind of sentence representation method? for example, the output of lstm, or cnn, or self-attention? since the key word \"encoder\" (and \"decoder\") was mentioned several times during the lecture, i want to figure out what it really means. thanks in advance!\n",
      "Expected Answer: any kind of sequence encoders: cnns, lstms, self-attention encoders or transformers (multi-head attention). note that cross-encoder architectures usually use attention encoders nowadays.\n",
      "\n",
      "Top most similar posts:\n",
      "['any kind of sequence encoders: cnns, lstms, self-attention encoders or transformers (multi-head attention). note that cross-encoder architectures usually use attention encoders nowadays.', 'we talked about that in the \"subword representation, contextualized representation\" lecture. please refer to those slides.', 'is brill tagger a discriminative model or generative model?.', 'quiz. 9 - neural language models use word-embedding models in their training..hi , when taught in class it was mentioned that a by product of neural language models was the word embeddings but i do not remember word embeddings to be involved in training. please can someone confirm this. thanks']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: vanishing gradient.do all models that use gradient descent, have a vanishing gradient problem?\n",
      "Expected Answer: it depends on the depth of the neural networks. consider a one-layer nn, there is no vanishing gradient problem. however, for a very depth model (consider a 100-layer fully connected nn), it is higher chance that there is a vanishing gradient problem.\n",
      "\n",
      "Top most similar posts:\n",
      "['it depends on the depth of the neural networks. consider a one-layer nn, there is no vanishing gradient problem. however, for a very depth model (consider a 100-layer fully connected nn), it is higher chance that there is a vanishing gradient problem.', 'not using grad.zero() to initialize the gradients after each iteration?.if we do not use grad.zero() and allow the gradients to accumulate, can it be useful in some cases?', 'gradient descent is a general optimization algorithm and works for all functions that are differentiable. so, you can use it for many problems. it may be possible to come up with a solution for perception using gradient descent, too. perceptron algorithm is specific to the perception model.', 'gradient descent vs perceptron algorithms.i want to verify my understanding is correct. the perceptron algorithm describes the full process of assigning random weights, then viewing examples from training data, adjusting weights, etc. gradient descent is a specific optimization technique that can be used to adjust weights. so one could replace the \"vanilla\" method of adjusting weights from the perceptron algorithm with gradient descent?']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: query regarding rnns.do rnns have a fixed vocabulary limitation ?\n",
      "Expected Answer: depends. an rnn is simply a sequence model. if your input can handle oov tokens (eg. passing fasttext embs to rnn), then then the answer is no. if you are simply using something like word2vec or one hot then yes.\n",
      "\n",
      "Top most similar posts:\n",
      "['depends. an rnn is simply a sequence model. if your input can handle oov tokens (eg. passing fasttext embs to rnn), then then the answer is no. if you are simply using something like word2vec or one hot then yes.', 'input to rnn.will the input of the rnn be a word embedding aka a vector of size (300,)? if yes then would not hidden state also have to be 300 ?', 'i second this! compared to fnn, rnns are know for capturing long term dependencies. the question does not specify whether rnns being compared to lstms or transformers.', 'the key is the word \"efficient\". rnn can model the long-term dependencies but computationally the process is not efficient and that is why gated rnn is developed.']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: convex optimization problem.hello,can you explain what is meant by saying an optimization problem is convex? and how can we actually determine whether an optimization problem is convex?\n",
      "Expected Answer: convex optimization means you are minimize a convex function. if you minimize a convex function, each local minima is global minima. for example, if you want to use a quadratic function to fit a series of data points, choosing the parameter of the quadratic function is a convex optimization problem.\n",
      "\n",
      "Top most similar posts:\n",
      "['convex optimization means you are minimize a convex function. if you minimize a convex function, each local minima is global minima. for example, if you want to use a quadratic function to fit a series of data points, choosing the parameter of the quadratic function is a convex optimization problem.', 'gradient descent is a general optimization algorithm and works for all functions that are differentiable. so, you can use it for many problems. it may be possible to come up with a solution for perception using gradient descent, too. perceptron algorithm is specific to the perception model.', 'gradient descent vs perceptron algorithms.i want to verify my understanding is correct. the perceptron algorithm describes the full process of assigning random weights, then viewing examples from training data, adjusting weights, etc. gradient descent is a specific optimization technique that can be used to adjust weights. so one could replace the \"vanilla\" method of adjusting weights from the perceptron algorithm with gradient descent?', 'ok, i thought the one more meaning you mentioned in class is the definition. from the explanation you gave, it makes more sense! thank you.']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: naive bayes.we are adding logarithmic probabilities instead of multiplying conditional probabilities because it can result in floating-point underflow. what is floating-point underflow? any example?\n",
      "Expected Answer: since probabilities lie between 0 and 1, multiplying a lot of them can result in very small numbers that the data type cannot represent. arithmetic underflow can occur when the true result of a floating point operation is smaller in magnitude (that is, closer to zero) than the smallest value representable as a normal floating point number in the target datatype.[1] arithmetic underflow - wikipedia floating point - what is the range of values a float can have in python? - stack overflow in naive bayes, i think it would affect the comparison, as probabilities that are actually different would end up being 0 and look equal.\n",
      "\n",
      "Top most similar posts:\n",
      "['since probabilities lie between 0 and 1, multiplying a lot of them can result in very small numbers that the data type cannot represent. arithmetic underflow can occur when the true result of a floating point operation is smaller in magnitude (that is, closer to zero) than the smallest value representable as a normal floating point number in the target datatype.[1] arithmetic underflow - wikipedia floating point - what is the range of values a float can have in python? - stack overflow in naive bayes, i think it would affect the comparison, as probabilities that are actually different would end up being 0 and look equal.', 'we will cover this topic when we cover language models. in short, because raw probability values are between 0 and 1 and does not convey an intuitive measure.', 'tf-idf.why do we use a logarithm for idf? how does it benefit mathematics', 'smoothing was covered in the first week of class, when talking about naive bayes. you can see descriptions of the process in jurafsky and martin, chapter 4, pages 5–6, and in manning, raghavan and schutze, chapter 13, page 260. smoothing for hmm works exactly the same way. you just need to make sure to separately smooth each row in the matrix, since each row represents an independent probability distribution. you are allowed to consult references. what is not allowed is consulting implementations (or texts that specifically describe an implementation).']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: logistic regression robustness to correlated features.in the logistic regression reading material, under the section 5.2.4 (choosing a classifier), it says logistic regression is more robust to correlated features : \"if two features f1 and f2 are perfectly correlated, regression will simply assign part of the weight to w1 and part to w2\" . is not the test for correlated features (test for multi-collinearity) an important step that we check before building the model ? as, the independence of two features is very important criteria in finding the partial gradient ??\n",
      "Expected Answer: generally yes. one should get rid of correlated features before fitting the model. however, it is always preferred if your model is robust to these without expert intervention.in addition, it is not always feasible to remove correlated features, e.g. due to limitations of the data or bias in the data. no training data is perfect, especially in nlp.\n",
      "\n",
      "Top most similar posts:\n",
      "['generally yes. one should get rid of correlated features before fitting the model. however, it is always preferred if your model is robust to these without expert intervention.in addition, it is not always feasible to remove correlated features, e.g. due to limitations of the data or bias in the data. no training data is perfect, especially in nlp.', 'is not it similar how we used word2vec features in coding assignment 2 ?', 'feature extraction and classifying using one model.since i use tfidf for feature extraction, i need to keep the bag of words in the model for feature extraction in classifying right? does that mean i also need to store the idf for each word calculated in training and load it for classification? since the testing data need the same idf for the words in my bag of words. (unseen words will be discarded)', 'vanishing gradient.do all models that use gradient descent, have a vanishing gradient problem?']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: emojis.are emojis considered to be part of nlp? it feels like it would be a relevant factor — such as for sentiment analysis.\n",
      "Expected Answer: not sure if i understand the question. so please correct me if i missed it. is your question \"can processing emojis be done as part of nlp? especially for your assignment.\" then yes it can be. just think of what you want to do with them as i have seen several research papers and projects on the topic. however, if you are asking \"are emojis a natural language?\". here is my opinion and others can join the discussion as well. i believe yes, emojis are a natural language. although they are manufactured originally, they have evolved naturally in human communities by repetition without conscious planning way beyond their original design. i found this interesting article on the topic:\n",
      "\n",
      "Top most similar posts:\n",
      "['not sure if i understand the question. so please correct me if i missed it. is your question \"can processing emojis be done as part of nlp? especially for your assignment.\" then yes it can be. just think of what you want to do with them as i have seen several research papers and projects on the topic. however, if you are asking \"are emojis a natural language?\". here is my opinion and others can join the discussion as well. i believe yes, emojis are a natural language. although they are manufactured originally, they have evolved naturally in human communities by repetition without conscious planning way beyond their original design. i found this interesting article on the topic:', 'nltk has a built-in function that you can use but you are free to use your own developed function if it leads to improved performance.', 'quiz. 9 - neural language models use word-embedding models in their training..hi , when taught in class it was mentioned that a by product of neural language models was the word embeddings but i do not remember word embeddings to be involved in training. please can someone confirm this. thanks', 'how to use word tokens as features.in the overview of code assignment 4, it said that \"you may use the word tokens as features. \". professor said i can count words instead of using tf-idf. can anybody give me an example about how to transfer a review (i.e. a sequence of words/strings) into a vector (a sequence of numbers)?']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: question 47.question 47 which of the following cannot be fine-tuned into an abstractive summarization model? 3 selected answer: @ transformer encoder-decoder answers: @ bert bart 15 transformer encoder-decoder\n",
      "Expected Answer: for abstractive sumarization you need an autoregressive model. bert is an mlm and does not do autoregressive generation.\n",
      "\n",
      "Top most similar posts:\n",
      "['question 47.question 47 which of the following cannot be fine-tuned into an abstractive summarization model? 3 selected answer: @ transformer encoder-decoder answers: @ bert bart 15 transformer encoder-decoder', 'question 47. bert cannot be fine-tuned to abstractive summarization because it could fill the masked words but cannot generate a summary with a decoder? thanks a lot!', 'question 32. can anyone explain why abstractive summarization could be a use case of nli? and for fact verification, we could have a context and a fact sentence, which we could do nli in my opinion? thank you very much!', 'question 32.question 32 which of the following can be a (eey=eeonn selected answer: @ fact verification answers: @ abstractive summarization fact verification named entity recognition none of the other choices', 'for abstractive sumarization you need an autoregressive model. bert is an mlm and does not do autoregressive generation.']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: question 32.question 32 which of the following can be a (eey=eeonn selected answer: @ fact verification answers: @ abstractive summarization fact verification named entity recognition none of the other choices\n",
      "Expected Answer: that should be indeed fact verification. i just corrected that in the system.\n",
      "\n",
      "Top most similar posts:\n",
      "['question 47.question 47 which of the following cannot be fine-tuned into an abstractive summarization model? 3 selected answer: @ transformer encoder-decoder answers: @ bert bart 15 transformer encoder-decoder', 'question 32. can anyone explain why abstractive summarization could be a use case of nli? and for fact verification, we could have a context and a fact sentence, which we could do nli in my opinion? thank you very much!', 'for question 8, the question is talking about the semantic structure and not the syntactic structure. hence, this will be false!', 'that should be indeed fact verification. i just corrected that in the system.']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: question 15.question 15 ‘syntactic ambiguity describes the situation where a string of words: 3 selected answer: ¢ can have more than one meaning answers: cannot be interpreted using syntactic rules can have more than one meaning @ can correspond to more than one structure can have dozens of implau\n",
      "Expected Answer: syntax is a matter of sentence structure, as explained in the introduction class, the parsing class, and the associated reading. while it is true that different structures often correspond to different meanings, this is not always the case. for example, \"(a cat and a dog) and a mouse\" has a very similar meaning to \"a cat and (a dog and a mouse)\".\n",
      "\n",
      "Top most similar posts:\n",
      "['question 15. i am confused about this question. in the class, the example of syntactic ambiguity is that \"flying planes can be dangerous\". and professor ron mentioned that words in this string have one meaning but the combination of words has more than one meaning. so i guess my choice also makes sense?', 'for question 8, the question is talking about the semantic structure and not the syntactic structure. hence, this will be false!', 'syntax is a matter of sentence structure, as explained in the introduction class, the parsing class, and the associated reading. while it is true that different structures often correspond to different meanings, this is not always the case. for example, \"(a cat and a dog) and a mouse\" has a very similar meaning to \"a cat and (a dog and a mouse)\".', \"quiz 8.question 8 0 out of 10 points one of the difference between rule-based and transfer-based machine translation is that transfer-based machine translation's rules on semantic structure is more generalizable. selected answer: €3 true answers: true @ false question 4 0 out of 10 points in ibm machine translation model, if there are k words in the source sentence and v words in target sentence in a pair in parallel corpus, how many possible alignments 13] exists in this pair. (a*b means a to the power of b) selected answer: ¢ (v+1)4k answers: k4v (v+41)4k @ (k+1)4v v‘k preriwll weuwviu vi lf = bllitreliaeinte prweieie analignment a is {a1,...@m} ,where a; € {0...1} hence there are (/ + 1)” possible alignments m-.. t{re nr arrrrfri\"]\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: exam question 26.question 26 0 out of 1 points if we plan to train a readability analysis model that decides how difficult a given paragraph reads, where the readability is scaled into ix] five levels {very easy, easy, normal, difficult, very difficult}, then which of the following model is not suitable for this task? selected answer: ¢ a multi-class classifier answers: @ aregressor with mae loss an ordinal regression model amulti-class classifier a label ranker\n",
      "Expected Answer: how would you train a sentiment classifier for movie reviews?\n",
      "\n",
      "Top most similar posts:\n",
      "['it should be 5 since it is a 5 class classification problem.', 'because ordinal regression is actually a classification method. it optimizes the probability of the class being leq than each ordinal label. how would you change this objective to mae?', 'question 47.question 47 which of the following cannot be fine-tuned into an abstractive summarization model? 3 selected answer: @ transformer encoder-decoder answers: @ bert bart 15 transformer encoder-decoder', 'the open/closed class distinction should not greatly improve or greatly decrease accuracy. remember, it only applies to unseen words, and there are not that many of those, so the effect should be rather small. from a good implementation of the open/closed class distinction you can expect an improvement of about 1 percentage point. if you see a great reduction in accuracy, then my first suspicion would be a bug in the implementation.']\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: quiz 8.question 8 0 out of 10 points one of the difference between rule-based and transfer-based machine translation is that transfer-based machine translation's rules on semantic structure is more generalizable. selected answer: €3 true answers: true @ false question 4 0 out of 10 points in ibm machine translation model, if there are k words in the source sentence and v words in target sentence in a pair in parallel corpus, how many possible alignments 13] exists in this pair. (a*b means a to the power of b) selected answer: ¢ (v+1)4k answers: k4v (v+41)4k @ (k+1)4v v‘k preriwll weuwviu vi lf = bllitreliaeinte prweieie analignment a is {a1,...@m} ,where a; € {0...1} hence there are (/ + 1)” possible alignments m-.. t{re nr arrrrfri\n",
      "Expected Answer: completely agree with you on transfer-based machine translation problem. for the ibm model problem, this is my understanding: in the ppt you screenshot, l is the length of the source and m is the length of the target, therefore (l+1)^m possible alignments. - for clarification: the slide states, \"[a]n alignment map determines which english word each french word originated from.\" meaning that the french word (target) came from the english word (source). (i honestly may be confusing this though.) in the quiz, k is the length of the source and v is the length of the target, therefore (k+1)^v possible alignments..\n",
      "\n",
      "Top most similar posts:\n",
      "['completely agree with you on transfer-based machine translation problem. for the ibm model problem, this is my understanding: in the ppt you screenshot, l is the length of the source and m is the length of the target, therefore (l+1)^m possible alignments. - for clarification: the slide states, \"[a]n alignment map determines which english word each french word originated from.\" meaning that the french word (target) came from the english word (source). (i honestly may be confusing this though.) in the quiz, k is the length of the source and v is the length of the target, therefore (k+1)^v possible alignments..', 'the question about parallel corpus for the language model, not mt.', 'question 15.question 15 ‘syntactic ambiguity describes the situation where a string of words: 3 selected answer: ¢ can have more than one meaning answers: cannot be interpreted using syntactic rules can have more than one meaning @ can correspond to more than one structure can have dozens of implau', 'is a parallel corpus required for statistical machine translation?.please correct me if i am wrong but as per my understanding the noisy channel model for machine translation being a statistical model does require a parallel corpus, right?']\n",
      "0.9135593220338984\n"
     ]
    }
   ],
   "source": [
    "top_k = 5\n",
    "sum_score = 0\n",
    "for i in range (0, len(evaluation_data['Question'])):\n",
    "    query = evaluation_data['Question'][i]\n",
    "    print(\"\\n\\n======================\\n\\n\")\n",
    "    print(\"Query:\", query)\n",
    "\n",
    "    if(not evaluation_data['AnswerList'][i]):\n",
    "      continue\n",
    "\n",
    "    expected_answer = evaluation_data['AnswerList'][i][0]\n",
    "\n",
    "    print(\"Expected Answer:\", expected_answer)\n",
    "\n",
    "    query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    # We use cosine-similarity and torch.topk to find the highest 5 scores\n",
    "    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]\n",
    "    top_results = torch.topk(cos_scores, k=top_k)\n",
    "\n",
    "    print(\"\\nTop most similar posts:\")\n",
    "    posts = []\n",
    "    for score, idx in zip(top_results[0], top_results[1]):\n",
    "      #Skip, if this post is already recommended\n",
    "      postLink=train_data.iloc[idx.item()]['Link']\n",
    "      # if post not in posts_seen:\n",
    "      #   posts_seen.add(post)\n",
    "      \n",
    "      if(int(score) != 1):\n",
    "        posts.append(train_data.iloc[idx.item()]['Sentence'])\n",
    "      #print(\"\\nPost: {}\\nDocument: {}\\nScore: {:.4f}\".format(postLink,,score))\n",
    "    print(posts)\n",
    "    \n",
    "    if(expected_answer == posts[0]):\n",
    "      sum_score+=1\n",
    "\n",
    "    if(expected_answer == posts[1]):\n",
    "      sum_score+=0.8\n",
    "\n",
    "    if(expected_answer == posts[2]):\n",
    "      sum_score+=0.6\n",
    "\n",
    "    if(expected_answer in posts):\n",
    "      sum_score+=0.5\n",
    "\n",
    "print(sum_score/len(evaluation_data['Question']))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
