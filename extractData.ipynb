{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import dateutil.parser\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEndorsedByStaff(endorsements):\n",
    "    for endorsement in endorsements:\n",
    "        if 'role' in endorsement and ('professor' in endorsement['role'] or 'instructor' in endorsement['role'] or 'ta' in endorsement['role']):\n",
    "            return True\n",
    "    \n",
    "def checkValidAnswer(post):\n",
    "    return ('i_answer' in post['type']) or ('tag_endorse' in post and isEndorsedByStaff(post['tag_endorse']))\n",
    "\n",
    "def getAnswerList(post):\n",
    "    answerList = []\n",
    "    if('children' in post):\n",
    "        postAnswers = post['children']\n",
    "        for postAnswer in postAnswers:\n",
    "            #whoAnswered = ''\n",
    "            answer = ''\n",
    "            if 'type' in postAnswer and checkValidAnswer(postAnswer) and 'history' in postAnswer and 'subject' not in postAnswer['history']:\n",
    "                #whoAnswered = postAnswer['type']\n",
    "                #last_modified = postAnswer['history'][len(postAnswer['history']) - 1]\n",
    "                last_modified = getLastModified(postAnswer)\n",
    "                answer = last_modified['content']\n",
    "                #answerToWhoAnswered = (answer, whoAnswered)\n",
    "                answerList.append(answer)\n",
    "    return answerList\n",
    "\n",
    "def getLastModified(post):\n",
    "    history = post['history']\n",
    "    last_modified_answer = history[0]\n",
    "    last_modified_datetime = dateutil.parser.parse(history[0]['created'])\n",
    "    for i in range(0, len(history)):\n",
    "        post_datetime = dateutil.parser.parse(history[i]['created'])\n",
    "        if(post_datetime > last_modified_datetime):\n",
    "            last_modified_datetime = post_date\n",
    "            last_modified_answer = history[i]\n",
    "            \n",
    "    return last_modified_answer\n",
    "\n",
    "def extractData(filename):\n",
    "    with open(filename, 'r') as openfile:\n",
    "        input = json.load(openfile)\n",
    "        #print(input)\n",
    "        df = pd.DataFrame(columns = ['Post', 'Sentence'])\n",
    "        for i in range(0, len(input)):\n",
    "            post = input[i]\n",
    "            if 'history' in post:\n",
    "                last_modified = getLastModified(post)\n",
    "                if 'subject' in last_modified and 'content' in last_modified and '<img' not in last_modified['content']:\n",
    "                    subject = last_modified['subject']\n",
    "                    content = last_modified['content']\n",
    "                    post_ID = post['nr']\n",
    "                    answerList = getAnswerList(input[i])\n",
    "                    df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)  \n",
    "                    for i in range(0, len(answerList)):\n",
    "                        df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
    "\n",
    "        return df\n",
    "#     for i in range(0, len(df)):\n",
    "#         print(\"Post: \"+str(int(df.iloc[i]['Post'])))\n",
    "#         print(\"Sentence: \"+df.iloc[i]['Sentence'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Post                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Sentence\n",
      "0   689                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Quiz. 9  - Neural language models use word-embedding models in their training..<p>Hi ,</p>\\n<p></p>\\n<p>When taught in class it was mentioned that a by product of neural language models was the word embeddings but I don&#39;t remember word embeddings to be involved in training. Please can someone confirm this.</p>\\n<p></p>\\n<p>Thanks</p>\\n<p></p>\n",
      "1   689                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     <p>Yes, word embedding are not used to rain Word2Vec. We only use one-hot vectors and then learn word embeddings as the weights of the two-layer network.</p>\n",
      "2   663                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Regarding random shuffling between epochs while training.<p>I&#39;m getting better Mean F1 scores for both vanilla and averaged perceptron without random shuffling the dataset between epochs while training. Is shuffling between epochs mandatory while training? Can I train the perceptron without shuffling the dataset?</p>\n",
      "3   663                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <p>Not mandatory, but see @661</p>\n",
      "4   638                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Feature extraction and classifying using one model.<p>Since I use TFIDF for feature extraction, I need to keep the bag of words in the model for feature extraction in classifying right? Does that mean I also need to store the IDF for each word calculated in training and load it for classification? Since the testing data need the same idf for the words in my bag of words. (unseen words will be discarded)</p>\n",
      "5   638                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        You could. Another option would be to put the IDF weights into your leaned weight vectors.\n",
      "6   608                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Feature vector for unseen words.<p>As we are creating one-hot encoding to convert reviews into features, how can I handle any unseen words in the test dataset? Should I skip the word? Or is there any other better alternative?</p>\\n<p></p>\n",
      "7   608                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Ignoring an unknown word is probably the simplest solution. Alternatively, you can devise features that could be triggered by both known and unknown words.\n",
      "8   579                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               smoothing transition probability matrix.<p>What is meant by smoothing each row in the transition probability matrix? If we&#39;re generally dividing by the number of times the previous tag is in the corpus, are we adding 1 to that number? </p>\n",
      "9   579                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Laplace smoothing was covered in the first week of class @490, and again last week (lecture 13 on October 4).\\n\\nSmoothing each row just means treating each outgoing transition separately. That is, we perform smoothing on all the transitions from noun; separately, we perform smoothing on all the transitions from verb; and so on. Because each of these is a separate probability distribution.\n",
      "10  541                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       HMM model.<p>Do we have to use forward-backward algorithm to learn the model as shown in the reference?</p>\n",
      "11  541                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  The forward-backward algorithm is for unsupervised learning. You don&#39;t need to do that, because the exercise provides labeled training data.\n",
      "12  536                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Smoothing.<p>Instead of Laplace Smoothing, I am trying to use additive smoothing. Still, I am not able to get good accuracy, leading not to be able to pinpoint the exact additive integer. Is there any other way to do this?</p>\n",
      "13  536                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Sophisticated smoothing could lead to a small improvement, but if the accuracy you’re getting is not in the general range of the baseline or reference, then you probably have a bug or error, and finding and fixing that would lead to more improvement than fiddling with the smoothing method\n",
      "14  589                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               How to use word tokens as features.<p>In the overview of Code Assignment 4, it said that &#34;You may use the word tokens as features. &#34;. Professor said I can count words instead of using TF-IDF. Can anybody give me an example about how to transfer a review (i.e. a sequence of words/strings) into a vector (a sequence of numbers)?</p>\n",
      "15  589  Using words as features, each word can be considered a feature, and the count of the word is the value. So a review like &#34;The Omni is the best hotel in the world&#34; will have the following feature values:\\n\\n{&#39;best&#39;: 1, &#39;hotel&#39;: 1, &#39;in&#39;: 1, &#39;is&#39;: 1, &#39;Omni&#39;: 1, &#39;the&#39;: 3, &#39;world&#39;: 1}\\n\\nIf you want to think of this as a fixed-length vector, then the length would be the entire vocabulary, and most values will be zero.\\n\\nNotice how in the feature structure above, I silently changed &#39;The&#39; to &#39;the&#39;, but kept &#39;Omni&#39; without change. This would be difficult to do programmatically, but this is one of the things to experiment with: Is it helpful to change case? Mess around with suffixes? Do something with punctuation? Is it helpful to develop features that aren&#39;t words? For example, use word bigrams as features? Are there any words that should be excluded from being used as features?\\n\\nI recommend starting with very simple features (say, just words), in order to get the basic logic of the perceptron working, Once you have that, you can experiment to see how changes to features affect performance of the perceptron.\\n\\nNo, word2vec is not allowed for this assignment.\n",
      "16  528                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Smoothing Emission Probability.<p>Why are we not smoothing emission probability?</p>\n",
      "17  528                                                                                                                                                                                                                                                                                                                                                                                                                     It&#39;s a tradeoff of runtime and accuracy. My best recommendation is to try it both ways: implement your model with and without smoothing on the emissions, measure runtime and accuracy for both options, and decide if the tradeoff is worthwhile.\\n\\nLooking at the theory, smoothing the emission probabilities is expected to cause a substantial increase in runtime, because now you&#39;re calculating paths into every tag for every word, instead of throwing away most of the paths, It also has the potential for an improvement in accuracy, since you may run into a situation where context strongly suggests a tag but the current word was only seen with other tags. However, the improvement in accuracy is likely to be fairly modest, because if a word was seen with some tags but not others, then either the word is rare or the other tags are rare for that word.\n",
      "18  511                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            States vs Observations.<p>The parts of speech themselves are the states correct? Which would make each word the observation?</p>\\n<p></p>\\n<p>I&#39;m confused because there&#39;s a possibility our input data doesn&#39;t contain all possible part of speech tags and therefore will always incorrectly label any words belonging to that part of speech. Is there a known list of part of speech tags we&#39;re supposed to use? Or just use the ones from the training data? </p>\n",
      "19  511                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      <p>Yes and Yes.</p>\\n<p></p>\\n<p>I am not sure if the data we are working with will have such cases. But if we do, then you are correct. It can happen that we do not see a tag in train dataset because of which we assign words belonging to that tag incorrectly when tagging test dataset.</p>\\n<p></p>\\n<p>No there is no known list of part of speech tags. We should be using and learning model from the ones that are present in training data.</p>\n",
      "20  506                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Is Brill tagger a discriminative model or generative model?.\n",
      "21  506                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             <p>Neither. The model does not output probabilities of tags or sequences of tags.</p>\n",
      "22  497                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Finding open-class parts of speech.<p>I am trying to find a good way to determine if a part of speech can be considered open-class. Are there any general rules to follow when doing this? I tried calculating the average vocabulary size of each part of speech, and then classifying everything with a greater than average vocab size as open-class. However, this greatly reduced the algorithms accuracy, so I suspect this is the wrong way to go about it.</p>\n",
      "23  497                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        The open/closed class distinction should not greatly improve or greatly decrease accuracy. Remember, it only applies to unseen words, and there aren&#39;t that many of those, so the effect should be rather small. From a good implementation of the open/closed class distinction you can expect an improvement of about 1 percentage point.\\n\\nIf you see a great reduction in accuracy, then my first suspicion would be a bug in the implementation.\n",
      "24  490                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           HMM Smoothing.<p>Hi!</p>\\n<p></p>\\n<p>I went through the algorithm in the references given to us and although there are tables showing the Viterbi trellis for an example with and without smoothing, I don&#39;t particularly see how this smoothing is done. Are we allowed to look for references on how smoothing is applied to the computation of transition probabilities or can you point some of the references we are allowed to use to this end?</p>\\n<p></p>\\n<p>Thanks!</p>\n",
      "25  490                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             <p>Smoothing was covered in the first week of class, when talking about Naive Bayes. You can see descriptions of the process in <a href=\"https://web.stanford.edu/~jurafsky/slp3/4.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Jurafsky and Martin, chapter 4, pages 5–6</a>, and in <a href=\"https://nlp.stanford.edu/IR-book/pdf/13bayes.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">Manning, Raghavan and Schutze, chapter 13, page 260</a>. Smoothing for HMM works exactly the same way. You just need to make sure to separately smooth each row in the matrix, since each row represents an independent probability distribution.</p>\\n<p></p>\\n<p>You are allowed to consult references. What is not allowed is consulting implementations (or texts that specifically describe an implementation).</p>\n",
      "26  484                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Question About Initial Probabilties.<p>Are initial probabilities calculated considering only the first word in a sentence, or do we use the Markov Assumption and consider each word in the sentence as a potential starting point? If it is the latter, then would every word in the sentence be considered as a potential starting point except the very last word in the sentence?</p>\n",
      "27  484                                                                                                                                                                                                                                                                                  <p>By initial probabilities, I assume you mean what Jurafsky and Martin call “an initial probability distribution over states” (<a href=\"https://web.stanford.edu/~jurafsky/slp3/8.pdf\" target=\"_blank\" rel=\"noopener noreferrer\">top of page 9</a>). This is the probability distribution of the first state in a sentence. To estimate the initial probability of state X, just take the number of sentences that start with X, and divide by the total number of sentences. Don&#39;t forget to apply smoothing.</p>\\n<p></p>\\n<p>An alternative approach is the algorithm presented in the written exercises that were distributed on Blackboard. Add a special initial state that is not associated with any observations, and assume that each sentence starts with a transition from that initial state to the first word-producing state. Then the initial probability distribution from the previous paragraph becomes just another row in the transition matrix. The two approaches are mathematically equivalent.</p>\n",
      "28  443                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Can anyone explain Q1?.<p>Why <strong>Chi-squared test</strong> is the most suitable hypothesis test to know if gender has anything to do with political party preference (Democrats, Republicans, etc)? </p>\\n<p>Why not t-test? What is the difference between Chi-squared test and t-test?</p>\n",
      "29  443                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <p>The chi-squared test is used when we have categorical variables (here gender and political party preference). However, the t-test is used when we have quantitative variables (like to compare the mean of 2 samples). </p>\n",
      "30  348                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Using cuda.<p>When implementing FNN and RNN, do we have to cast tensors to a cuda type to run on GPU? Thank you!</p>\n",
      "31  348                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Yes\n",
      "32  254                                                                                                                                                                                                                                                                                                                                                    Quiz Question - 9.<p>Hi,</p>\\n<p></p>\\n<p>Question 9:</p>\\n<p>Choice &#39;It&#39;s efficient to learn long-term dependencies through time&#39; is considered as a wrong option in the rubrics. </p>\\n<p></p>\\n<p>But RNN&#39;s are known for Long Term Dependencies over time through their dynamic system. Normalized RNN&#39;s are even more capable of unfolding long term dependences over time.</p>\\n<p>Here are some relevant papers:</p>\\n<p>1. <a href=\"https://www.researchgate.net/publication/225200516_Learning_Long_Term_Dependencies_with_Recurrent_Neural_Networks\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.researchgate.net/publication/225200516_Learning_Long_Term_Dependencies_with_Recurrent_Neural_Networks</a></p>\\n<p>2. <a href=\"https://arxiv.org/abs/1611.05216\" target=\"_blank\" rel=\"noopener noreferrer\">https://arxiv.org/abs/1611.05216</a></p>\\n<p>3. http://proceedings.mlr.press/v139/rusch21a/rusch21a.pdf</p>\n",
      "33  254                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     The key is the word &#34;efficient&#34;. RNN can model the long-term dependencies but computationally the process is not efficient and that&#39;s why gated RNN is developed.\n",
      "34  245                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Not using grad.zero() to initialize the gradients after each iteration?.<p>If we do not use grad.zero() and allow the gradients to accumulate, can it be useful in some cases?</p>\n",
      "35  245                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           I haven&#39;t seen such a case personally but likely there is a reason behind the PyTorch designer team that set it to be accumulative.\n",
      "36  241                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Vague Quiz 3 Question.<p>During the quiz there was this T/F question that I felt was vague:<br /><br />In RNN, the weights during each time step is shared.<br /><br />I thought that this question was asking if the neurons are using the same weights at each time step, which I thought was false since we should be updating the weights and hence, using different weights at each time step.</p>\\n<p></p>\\n<p><br />The correct answer for this question was True.</p>\\n<p>I believe that this question is asking whether we are calculating the new weight at time t using the weights from time t-1.</p>\\n<p></p>\\n<p>Am I the only who felt that the question was a little misleading?</p>\n",
      "37  241                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <p>Weights during RNN backprop aren&#39;t updated on each time step, since backprop happens after the full forward computation is done (I believe from slide 9 of lecture 6). That means that the same weights would actually be shared. Therefore, I do not think this question was misleading.</p>\\n<p></p>\\n<p>(For RNNs, a &#34;timestep&#34; refers to the position of an individual RNN cell, not a full forward-backward computation.)</p>\\n<p></p>\n",
      "38  227                                                                                                                                                                                                            Cosine Similarity numerical.<p>I just wanted to know if this is correct</p>\\n<p></p>\\n<p><strong>Question : Find cosine similarity for :</strong></p>\\n<p><strong>x1 = (3,4)</strong></p>\\n<p><strong>x2 = (1,0)</strong></p>\\n<p><strong></strong></p>\\n<p><strong>Solution : </strong></p>\\n<p>                                  x*y</p>\\n<p>Cosine Similarity = ________</p>\\n<p>                               ||x|| ||y||</p>\\n<p></p>\\n<p>3*1 &#43; 4*0</p>\\n<p>___________________________ = 3/5 = 0.6</p>\\n<p>sqrt((3*3&#43;4*4)x(1*1&#43;0*0))</p>\\n<p></p>\\n<p>Can you please confirm this calculation or is it incomplete as</p>\\n<p>do we consider the <strong>Euclidean Distance </strong>instead, by assuming center to be the origin?</p>\\n<p></p>\\n<p>(3-0)*(1-0) &#43; (4-0)*(0-0)</p>\\n<p>___________________________ = 3/2 = 0.6</p>\\n<p>sqrt((3*3&#43;4*4)x(1*1&#43;0*0))</p>\\n<p></p>\\n<p></p>\\n<p><strong>Do we consider the center to be (0,0) always, as it was not mentioned by default in the question in the Quiz?</strong></p>\\n<p><strong></strong></p>\\n<p>Thank you</p>\n",
      "39  227                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             The first computation is correct and the definitions of dot product and vector norm are both independent of the center of coordinate.\n",
      "40   85                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             TF-IDF.<p>Should I run tf-idf and extract features treating entire dataset as corpus? Or by doing it classwise?? </p>\n",
      "41   85                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <p>You should do it on the entire dataset.</p>\n",
      "42   84                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Gradient Descent vs Perceptron Algorithms.<p>I want to verify my understanding is correct. </p>\\n<p></p>\\n<p>The perceptron algorithm describes the full process of assigning random weights, then viewing examples from training data, adjusting weights, etc. </p>\\n<p></p>\\n<p>Gradient descent is a specific optimization technique that can be used to adjust weights.</p>\\n<p></p>\\n<p>So one could replace the &#34;vanilla&#34; method of adjusting weights from the perceptron algorithm with gradient descent? </p>\n",
      "43   84                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <p>Gradient descent is a general optimization algorithm and works for all functions that are differentiable. So, you can use it for many problems. It may be possible to come up with a solution for perception using gradient descent, too. Perceptron algorithm is specific to the perception model.</p>\n",
      "44   79                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Lemmatization based on pos tag.<p>the ntlk lemmatizer will not lemmatize &#34;wearing&#34; to &#34;wear&#34; unless explicit mentioned to considered it as a word, because the default tag is &#34;noun&#34; . Is this lemmatization acceptable?</p>\n",
      "45   79                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             <p>I think so. You don&#39;t have to look at the specific cases of lemmatization otherwise it would be too much work, the default should suffice.</p>\n",
      "46   58                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       Why do we use perplexity and not raw probability to evaluate language models?.Why do we use perplexity and not raw probability to evaluate language models?\n",
      "47   58                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             <p>We will cover this topic when we cover language models. In short, because raw probability values are between 0 and 1 and does not convey an intuitive measure.</p>\n",
      "48   39                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  BoW Dictionary.<p>Does the dictionary need to be ordered in some way for Bow to be used ? If so, is there any advantage to ordering it in alphabetical way ?</p>\n",
      "49   39                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  <p>ordering shouldn&#39;t have any effect when building BOW. unless you are dealing with billions of reviews, even in such case ordering your BOW w.r.t word frequency( to reduce document traversal) might not have any significant effect.</p>\n",
      "50   20                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Bag of words.How is a model which encounters a vector space representation of word Vs document which is extremely sparse affected? What can be done to handle the negative effects on the model?\n",
      "51   20                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <p>In this case, the feature might not be a good representation of the document. You can expand the vocabulary to make the representation less sparse, at the cost of increasing the dimension of the vector.</p>\\n<p></p>\\n<p>BoW is a simple model, it suffers from the problems like the one you mentioned inherently. It&#39;s useful in some simple applications, when you don&#39;t have to deal with the outliers. </p>\n",
      "52   22                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         TF-IDF.Why do we use a logarithm for idf? How does it benefit mathematics\n",
      "53   22                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <p>It suppresses large values to avoid making some feature values that correspond to infrequent ones too large. In practice, this has impression been found to be helpful.</p>\n",
      "54   37                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Tokenize Review before Lemmatization.<p>Should we tokenize review_body before lemmatizing the sentence? </p>\n",
      "55   37                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          <p>Lemmatization is performed on a word-level basis.</p>\n",
      "56   86                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            contractions library.<p>Do yall have any other recommendations besides the standard contractions library (with which you can do contractions.fix)? I noticed online that there are issues with it when dealing with ambiguity in tense and all. OR do yall think this can be safely ignored with respect to the performance of the various models?</p>\n",
      "57   86                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <p>The standard library should be fine for a decent performance but if you think you can do better, you can use an improved approach on top of it.</p>\n",
      "58  101                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      Performing lemmatization on the whole of reviews dataset.<p>The lemmatization process is taking a long time. Has anyone faced this issue? Anything that can done to improve the runtime?</p>\n",
      "59  101                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   <p>Running time depends on your machine, too. But generally, it is not superfast and we don&#39;t penaliz you if your running time is long.</p>\n",
      "60  116                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Different preprocessing steps for each Algorithm.<p>Hello,</p>\\n<p></p>\\n<p>Can we have different preprocessing steps for different algorithms given we explain the steps in detail in report and including/excluding some preprocessing steps improved scores.</p>\\n<p></p>\\n<p>Or is it required that all data go through same set of preprocessing steps and then each algorithm is evaluated on it?</p>\\n<p></p>\\n<p></p>\n",
      "61  116                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Yes, that is possible.\n",
      "62  317                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  What is the number of FNN output layer.<p>Like we set first hidden layer as 50, the second hidden layer is 10. But how about the output layer? Is it 5 or 1?</p>\n",
      "63  317                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   It should be 5 since it&#39;s a 5 class classification problem.\n",
      "64  347                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          Input to RNN.<md>Will the input of the RNN be a word embedding aka a vector of size (300,)? if yes then wouldn't hidden state also have to be 300 ?</md>\n",
      "65  347                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <p>I suppose the hidden layer to have a shape like (num_layers, time_steps(i.e. 20), output_embedding) now the output embedding can be of your choice. Just like the hidden layer output dimensions of MLp</p>\n",
      "66  358                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Training on rnn and gru too slow even using gpu.<p>Do we need to iterate through entire training dataset in one epoch? It takes about 1 hr to run 10 epoch for rnn which I directly follow the tutorial in the pdf and add a optimizer.</p>\\n<p>But in the tutorial seems only train on one document for one epoch.</p>\\n<p>Even worse for gru which I wait 20 minutes and does not finish one epoch.</p>\\n<p>Is there suggestion to solve this problem?</p>\\n<p>by the way I choose hidden size as 30</p>\n",
      "67  362                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Does anyone have the same problem that rnn and gru loss does not decrease?.<p>I trained for 100 epoch and it keeps at a very high value same as beginning.</p>\\n<p>Stuck here and debug for hours and did not see a difference. Does anyone have the same problem? Any suggestion?</p>\n",
      "68  362                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               Maybe change the learning rate and see if it helps.\n",
      "0   594                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   How to handle OOVs?.<p>What are the different ways to handle OOV (out of vocabulary) words?</p>\n",
      "1   594                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         <p>We talked about that in the &#34;Subword representation, contextualized representation&#34; lecture. Please refer to those slides.</p>\n",
      "2   570                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           KNN classification K = 1.Is there a situation that using k = 1 is acceptable for KNN. If k=1 gives the highest model performance on an “unseen” dev set, does it make sense to use k=1?\n",
      "3   570                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         <p>In that case you don’t really need to think about using an ML model. If your real world data maps 1:1 to your train data, you might as well write a SQL(Elasticsearch would be better) query with training data as your database. </p>\n",
      "4   544                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Inter-rather agreement vs inter-annotator agreement.Hello,<div><br /></div><div>Are these two terms the same thing — just IAA being a bit more specified for annotation? Otherwise I’ve seen them be used interchangeably in many places, and are measures with the same method’s, like Cohen’s kappa. </div>\n",
      "5   544                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       <p>Same</p>\n",
      "6   487                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              TensorFlow or PyTorch?.<p>Hello,</p>\\n<p></p>\\n<p>TensorFlow and PyTorch seem to be the most popular ML libraries today. Is there any recommendation on what is more suitable for NLP tasks?</p>\\n<p></p>\\n<p>In terms of learning curve, industry-relevance, ease of deployment, etc.?</p>\\n<p></p>\\n<p>Thanks.</p>\n",
      "7   487                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   <p>Historically, PyTorch had a better performance on recursive networks (e.g. LSTM) that were dominant before transformers, but they are both pretty good now. I personally use PyTorch as it feels more natural to me but TF2.0 is also pretty mature and powerful once you get into the mindset. I cannot comment about industry usage. So just pick one.</p>\n",
      "8   382                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     POS Tagging: Preprocessing.Is preprocessing generally done with POS Tagging? Such as removing stop words, removing punctuations, or stemming to name a few.<div><br /></div><div>Thanks</div>\n",
      "9   382                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   I don&#39;t think we are supposed to do this because stopwords too have a part of speech associated with them. So we are needed to predict that. Also, these stopwords and predictions define the probability of certain POS, if you remove them you model will not take this into account. Certain POS can have more probability to end or start the sentence than others. Removing stopwords, punctuation etc will ignore this. \\n\\nI think stemming too shouldn&#39;t be done as it can change the POS tag. Please correct me if I am wrong.\n",
      "10  268                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Does encoder (decoder) imply a specific model or just a general speaking of sentence representation?.<p>In the class, prof. Chen talked about Bi-encoder &amp; Cross-encoder, and I am confused that if the encoder is a specific model or it could be any kind of sentence representation method? For example, the output of LSTM, or CNN, or self-attention? Since the key word &#34;encoder&#34; (and &#34;decoder&#34;) was mentioned several times during the lecture, I want to figure out what it really means.</p>\\n<p></p>\\n<p>Thanks in advance!</p>\n",
      "11  268                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         <p>Any kind of sequence encoders: CNNs, LSTMs, Self-attention encoders or Transformers (multi-head attention).</p>\\n<p>Note that cross-encoder architectures usually use attention encoders nowadays.</p>\n",
      "12  255                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Vanishing Gradient.Do all models that use gradient descent, have a vanishing gradient problem?\n",
      "13  255                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <p>It depends on the depth of the neural networks. Consider a one-layer NN, there is no vanishing gradient problem. However, for a very depth model (consider a 100-layer fully connected NN), it is higher chance that there is a vanishing gradient problem.</p>\n",
      "14  251                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Query regarding RNNs.<p>Do RNNs have a fixed vocabulary limitation ? </p>\n",
      "15  251                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <p>Depends. An RNN is simply a sequence model. If your input can handle OOV tokens (eg. passing fasttext embs to RNN), then then the answer is no.</p>\\n<p>If you’re simply using something like word2vec or one hot then yes.</p>\n",
      "16  243                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Convex Optimization Problem.Hello,<div><br /></div><div>Can you explain what is meant by saying an optimization problem is convex? And how can we actually determine whether an optimization problem is convex?</div>\n",
      "17  243                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               <p>Convex optimization means you are minimize a convex function. If you minimize a convex function, each local minima is global minima. For example, if you want to use a quadratic function to fit a series of data points, choosing the parameter of the quadratic function is a convex optimization problem.</p>\n",
      "18  103                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Naive Bayes.We are adding logarithmic probabilities instead of multiplying conditional probabilities because it can result in floating-point underflow.  What is floating-point underflow? Any example?\n",
      "19  103                        <p>Since probabilities lie between 0 and 1, multiplying a lot of them can result in very small numbers that the data type cannot represent.</p>\\n<p></p>\\n<blockquote>\\n<p>Arithmetic underflow can occur when the true result of a <a href=\"https://en.wikipedia.org/wiki/Floating-point_arithmetic\">floating point operation</a> is smaller in magnitude (that is, closer to zero) than the smallest value representable as a <a href=\"https://en.wikipedia.org/wiki/Normal_number_%28computing%29\">normal</a> floating point number in the target <a href=\"https://en.wikipedia.org/wiki/Data_type\">datatype</a>.<sup><a href=\"https://en.wikipedia.org/wiki/Arithmetic_underflow#cite_note-1\">[1]</a></sup></p>\\n</blockquote>\\n<p><sup></sup></p>\\n<p><sup><a href=\"https://en.wikipedia.org/wiki/Arithmetic_underflow\">Arithmetic underflow - Wikipedia</a></sup><sup></sup></p>\\n<p><sup><a href=\"https://stackoverflow.com/questions/1835787/what-is-the-range-of-values-a-float-can-have-in-python\">floating point - What is the range of values a float can have in Python? - Stack Overflow</a></sup></p>\\n<p><sup></sup></p>\\n<p>In Naive Bayes, I think it would affect the comparison, as probabilities that are actually different would end up being 0 and look equal.</p>\n",
      "20   54                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     Logistic Regression robustness to correlated features.<p>In the logistic regression reading material, under the section 5.2.4 (Choosing a classifier), it says logistic regression is more robust to correlated features : &#34;if two features f1 and f2 are perfectly correlated, regression will simply assign part of the weight to w1 and part to w2&#34; .</p>\\n<p></p>\\n<p>Isn&#39;t the test for correlated features (test for multi-collinearity) an important step that we check before building the model ? As, the independence of two features is very important criteria in finding the partial gradient ??</p>\n",
      "21   54                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       <p>Generally yes. One should get rid of correlated features before fitting the model. However, it is always preferred if your model is robust to these without expert intervention.<br />In addition, it is not always feasible to remove correlated features, e.g. due to limitations of the data or bias in the data. No training data is perfect, especially in NLP.</p>\n",
      "22   18                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Emojis.<div>Are emojis considered to be part of NLP? It feels like it would be a relevant factor — such as for sentiment analysis. </div>\n",
      "23   18                                                                                                                                                                                                                                                                                                                                                                                                             <p>Not sure if I understand the question. So please correct me if I missed it.</p>\\n<p></p>\\n<p>Is your question &#34;<strong>can processing emojis be done as part of NLP?</strong> especially for your assignment.&#34; Then yes it can be. Just think of what you want to do with them as I have seen several research papers and projects on the topic.</p>\\n<p></p>\\n<p>However, if you are asking &#34;<strong>are emojis a natural language</strong>?&#34;. Here is my opinion and others can join the discussion as well. I believe yes, emojis are a natural language. Although they are manufactured originally, they have evolved naturally in human communities by repetition without conscious planning way beyond their original design. I found this interesting article on the topic: https://www.clarabridge.com/blog/debunking-natural-language-processing-emojis-and-emoticons</p>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:51: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': subject + \".\" + content}, ignore_index = True)\n",
      "/var/folders/p8/fbqgjwlx683072z049myq84r0000gn/T/ipykernel_15786/1192936527.py:53: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append({'Post': post_ID, 'Sentence': answerList[i]}, ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "#usage\n",
    "final_df = pd.concat([extractData(\"data/fall_22_nlp.json\"), extractData(\"data/spring_22_nlp.json\")])\n",
    "print(final_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to Lowercase\n",
    "final_df[\"Sentence\"] = final_df[\"Sentence\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Remove all HTML tags\n",
    "final_df[\"Sentence\"] = final_df[\"Sentence\"].apply(lambda x: BeautifulSoup(str(x)).get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#Remove all URLs\n",
    "final_df[\"Sentence\"] = final_df[\"Sentence\"].apply(lambda x: re.sub(r'\\s*(https?://|www\\.)+\\S+(\\s+|$)', \" \", str(x), flags=re.UNICODE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove extra spaces\n",
    "final_df[\"Sentence\"] = final_df[\"Sentence\"].apply(lambda x: re.sub(r\"\\s+\", \" \", str(x), flags=re.UNICODE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "\n",
    "#Removing all contractions\n",
    "def perform_contractions(series):\n",
    "    series = series.apply(lambda x: contractions.fix(x))\n",
    "    return series\n",
    "\n",
    "x = perform_contractions(final_df[\"Sentence\"])\n",
    "final_df[\"Sentence\"] = x\n",
    "\n",
    "#Removing uppercase letters which might be introduced after removing contractions\n",
    "final_df[\"Sentence\"] = final_df[\"Sentence\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     quiz. 9 - neural language models use word-embe...\n",
       "1     yes, word embedding are not used to rain word2...\n",
       "2     regarding random shuffling between epochs whil...\n",
       "3                           not mandatory, but see @661\n",
       "4     feature extraction and classifying using one m...\n",
       "                            ...                        \n",
       "19    since probabilities lie between 0 and 1, multi...\n",
       "20    logistic regression robustness to correlated f...\n",
       "21    generally yes. one should get rid of correlate...\n",
       "22    emojis.are emojis considered to be part of nlp...\n",
       "23    not sure if i understand the question. so plea...\n",
       "Name: Sentence, Length: 93, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final_df[\"Sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Post</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>689</td>\n",
       "      <td>quiz. 9 - neural language models use word-embe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>689</td>\n",
       "      <td>yes, word embedding are not used to rain word2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>663</td>\n",
       "      <td>regarding random shuffling between epochs whil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>663</td>\n",
       "      <td>not mandatory, but see @661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>638</td>\n",
       "      <td>feature extraction and classifying using one m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>103</td>\n",
       "      <td>since probabilities lie between 0 and 1, multi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>54</td>\n",
       "      <td>logistic regression robustness to correlated f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>54</td>\n",
       "      <td>generally yes. one should get rid of correlate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>18</td>\n",
       "      <td>emojis.are emojis considered to be part of nlp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>18</td>\n",
       "      <td>not sure if i understand the question. so plea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Post                                           Sentence\n",
       "0   689  quiz. 9 - neural language models use word-embe...\n",
       "1   689  yes, word embedding are not used to rain word2...\n",
       "2   663  regarding random shuffling between epochs whil...\n",
       "3   663                        not mandatory, but see @661\n",
       "4   638  feature extraction and classifying using one m...\n",
       "..  ...                                                ...\n",
       "19  103  since probabilities lie between 0 and 1, multi...\n",
       "20   54  logistic regression robustness to correlated f...\n",
       "21   54  generally yes. one should get rid of correlate...\n",
       "22   18  emojis.are emojis considered to be part of nlp...\n",
       "23   18  not sure if i understand the question. so plea...\n",
       "\n",
       "[93 rows x 2 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
